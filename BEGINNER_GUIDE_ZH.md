# è¯¦ç»†åˆå­¦è€…æŒ‡å— - åŸºäºRAGçš„WebèŠå¤©åº”ç”¨ç¨‹åº

> ğŸ‘‹ **å…³äºæœ¬æŒ‡å—**: ä¸“ä¸ºåˆå­¦è€…AIå·¥ç¨‹å¸ˆè®¾è®¡ï¼Œé€šè¿‡æ­¤ä»£ç åº“å­¦ä¹ ç°ä»£AIæŠ€æœ¯æ ˆã€‚

## ğŸ¯ ä½ èƒ½ä»è¿™ä¸ªé¡¹ç›®ä¸­å­¦åˆ°ä»€ä¹ˆ

### æŠ€æœ¯é¢†åŸŸ
- **RAG (Retrieval-Augmented Generation)**: AIèŠå¤©çš„æœ€æ–°æ–¹æ³•
- **å‘é‡æ•°æ®åº“**: å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£çš„å…³é”®æŠ€æœ¯
- **å…¨æ ˆå¼€å‘**: React + FastAPI çš„ç°ä»£ç»„åˆ
- **å®¹å™¨æŠ€æœ¯**: ä½¿ç”¨Dockerè¿›è¡Œå®ç”¨çš„å¼€å‘ç¯å¢ƒè®¾ç½®
- **LLMè¿ç»´**: ä½¿ç”¨Ollamaè¿›è¡Œé«˜æ•ˆçš„æ¨¡å‹ç®¡ç†

### æŠ€èƒ½ä¹ å¾—ç›®æ ‡
1. **AIæŠ€æœ¯çš„å®è·µç†è§£**: ä»è¿è¡Œçš„ç³»ç»Ÿä¸­å­¦ä¹ ï¼Œè€Œä¸ä»…ä»…æ˜¯ç†è®º
2. **ç°ä»£Webå¼€å‘**: ä½¿ç”¨TypeScript + Reactè¿›è¡Œæœ€æ–°çš„å‰ç«¯å¼€å‘
3. **APIè®¾è®¡**: ä½¿ç”¨FastAPIè®¾è®¡å’Œå®ç°REST API
4. **æ•°æ®åº“è®¾è®¡**: å‘é‡æ•°æ®åº“çš„æ¦‚å¿µå’Œåº”ç”¨
5. **DevOpsåŸºç¡€**: ä½¿ç”¨Dockerå®ç°å¼€å‘ç¯å¢ƒçš„æ ‡å‡†åŒ–

## ğŸ“š æŒ‰çŸ¥è¯†æ°´å¹³åˆ’åˆ†çš„å­¦ä¹ è·¯å¾„

### ğŸŸ¢ çº§åˆ«1: ç¼–ç¨‹åˆå­¦è€…
**æ‰€éœ€åŸºç¡€çŸ¥è¯†**: åŸºæœ¬çš„ç¼–ç¨‹æ¦‚å¿µï¼ˆå˜é‡ã€å‡½æ•°ã€æ¡ä»¶è¯­å¥ï¼‰

**å­¦ä¹ é¡ºåº**:
1. [åº”ç”¨ç¨‹åºæ“ä½œæ£€æŸ¥](#step1-æ“ä½œæ£€æŸ¥)
2. [ç†è§£åŸºæœ¬æ¦‚å¿µ](#åŸºæœ¬æ¦‚å¿µè§£è¯´)
3. [æŒæ¡æ–‡ä»¶ç»“æ„](#é¡¹ç›®ç»“æ„è¯¦è§£)
4. [ç®€å•å®šåˆ¶](#åˆå­¦è€…å®šåˆ¶)

### ğŸŸ¡ çº§åˆ«2: æœ‰ç»éªŒçš„Webå¼€å‘è€…
**å…ˆå†³æ¡ä»¶**: HTML/CSS/JavaScriptåŸºç¡€ï¼ŒREST APIæ¦‚å¿µ

**å­¦ä¹ é¡ºåº**:
1. [æŠ€æœ¯æ ˆæ¦‚è¿°](#æŠ€æœ¯æ ˆè¯¦è§£)
2. [ç†è§£æ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [å‰ç«¯åˆ†æ](#å‰ç«¯è¯¦è§£)
4. [åç«¯APIåˆ†æ](#åç«¯apiè¯¦è§£)

### ğŸŸ  çº§åˆ«3: AI/MLåˆå­¦è€…
**å…ˆå†³æ¡ä»¶**: Pythonï¼ŒåŸºæœ¬çš„æœºå™¨å­¦ä¹ æ¦‚å¿µ

**å­¦ä¹ é¡ºåº**:
1. [ç†è§£RAGæ¶æ„](#ragæŠ€æœ¯è¯¦è§£)
2. [åˆ©ç”¨å‘é‡æ•°æ®åº“](#å‘é‡æ•°æ®åº“è¯¦è§£)
3. [LLMé›†æˆæ¨¡å¼](#llmé›†æˆæ¨¡å¼)
4. [åº”ç”¨ç³»ç»Ÿè®¾è®¡](#åº”ç”¨ç³»ç»Ÿè®¾è®¡)

## Step 1: æ“ä½œæ£€æŸ¥

### 1.1 è¯¦ç»†ç¯å¢ƒè®¾ç½®

#### macOSç¯å¢ƒä¸‹çš„è¯¦ç»†æ­¥éª¤

```bash
# 1. å®‰è£…Homebrewï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# 2. å®‰è£…å¿…è¦çš„å·¥å…·
brew install git docker ollama

# 3. å®‰è£…Docker Desktopï¼ˆéœ€è¦GUIï¼‰
open https://www.docker.com/products/docker-desktop/

# 4. å…‹éš†ä»£ç åº“
git clone <æ­¤ä»£ç åº“URL>
cd simple-web-chat
```

#### Windowsç¯å¢ƒä¸‹çš„è¯¦ç»†æ­¥éª¤

```powershell
# 1. é€šè¿‡Chocolateyå®‰è£…å·¥å…·ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰
Set-ExecutionPolicy Bypass -Scope Process -Force
iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))

choco install git docker-desktop

# 2. æ‰‹åŠ¨å®‰è£…Ollama
# ä» https://ollama.ai/ ä¸‹è½½Windowsç‰ˆæœ¬

# 3. å…‹éš†ä»£ç åº“
git clone <æ­¤ä»£ç åº“URL>
cd simple-web-chat
```

### 1.2 é¦–æ¬¡å¯åŠ¨æ¸…å•

```bash
# âœ… ç¡®è®¤Docker Desktopæ­£åœ¨è¿è¡Œ
docker --version
docker compose version

# âœ… ç¡®è®¤Ollamaæ­£åœ¨è¿è¡Œ
ollama serve &
ollama list

# âœ… ä¸‹è½½æ‰€éœ€çš„æ¨¡å‹ï¼ˆè¿™éœ€è¦æ—¶é—´ï¼‰
ollama pull mxbai-embed-large    # ~669MB
ollama pull tinyllama            # ~638MBï¼ˆç”¨äºæµ‹è¯•ï¼‰

# âœ… å¯åŠ¨åº”ç”¨ç¨‹åº
docker compose up --build

# âœ… åœ¨æµè§ˆå™¨ä¸­è®¿é—®
open http://localhost:5173
```

### 1.3 æ“ä½œæ£€æŸ¥æ­¥éª¤

1. **ç½‘ç«™æå–æµ‹è¯•**
   ```
   ç¤ºä¾‹URL: https://zh.wikipedia.org/wiki/äººå·¥æ™ºèƒ½
   â†’ ç‚¹å‡» "Process URL"
   â†’ ç¡®è®¤æˆåŠŸæ¶ˆæ¯
   ```

2. **èŠå¤©åŠŸèƒ½æµ‹è¯•**
   ```
   é€‰æ‹©ä¸Šä¸‹æ–‡: æ‚¨åˆšåˆšæå–çš„URL
   ç¤ºä¾‹é—®é¢˜: "äººå·¥æ™ºèƒ½çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
   â†’ ç¡®è®¤AIæä¾›ç­”æ¡ˆ
   ```

## åŸºæœ¬æ¦‚å¿µè§£è¯´

### ä»€ä¹ˆæ˜¯RAG (Retrieval-Augmented Generation)ï¼Ÿ

**ä¼ ç»ŸèŠå¤©æœºå™¨äºº**:
```
ç”¨æˆ·é—®é¢˜ â†’ LLM â†’ ç­”æ¡ˆ
```
- é—®é¢˜: æ— æ³•å›ç­”LLMè®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰çš„ä¿¡æ¯
- é—®é¢˜: å¯èƒ½æä¾›è¿‡æ—¶æˆ–ä¸å‡†ç¡®çš„ä¿¡æ¯

**RAGç³»ç»Ÿ**:
```
ç”¨æˆ·é—®é¢˜ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ LLM (é—®é¢˜ + æ–‡æ¡£) â†’ ç­”æ¡ˆ
```
- è§£å†³æ–¹æ¡ˆ: åŸºäºæœ€æ–°çš„å‡†ç¡®æ–‡æ¡£å›ç­”
- è§£å†³æ–¹æ¡ˆ: å¯ä»¥åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†

### å‘é‡æ•°æ®åº“çš„ä½œç”¨

**ä¼ ç»Ÿæœç´¢**:
```sql
SELECT * FROM documents WHERE content LIKE '%äººå·¥æ™ºèƒ½%'
```
- é—®é¢˜: å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…çš„å…³é”®å­—ï¼Œåˆ™æ— æ³•æ‰¾åˆ°æ–‡æ¡£
- é—®é¢˜: æ— æ³•æ‰¾åˆ°è¯­ä¹‰ç›¸å…³çš„æ–‡æ¡£

**å‘é‡æœç´¢**:
```python
# ç†è§£â€œAIâ€ã€â€œæœºå™¨å­¦ä¹ â€å’Œâ€œæ·±åº¦å­¦ä¹ â€åœ¨è¯­ä¹‰ä¸Šæ˜¯æ¥è¿‘çš„
query_vector = embedding_model("å‘Šè¯‰æˆ‘å…³äºäººå·¥æ™ºèƒ½çš„ä¿¡æ¯")
similar_docs = vector_db.similarity_search(query_vector, top_k=5)
```

### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‰ç«¯         â”‚â”€â”€â”€â–¶â”‚   åç«¯API      â”‚â”€â”€â”€â–¶â”‚  Ollama (LLM)   â”‚
â”‚   (React/TS)  â”‚    â”‚   (FastAPI)   â”‚    â”‚  (ä¸»æœº)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Milvuså‘é‡æ•°æ®åº“ â”‚
                â”‚   (Docker)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## é¡¹ç›®ç»“æ„è¯¦è§£

### ç›®å½•ç»“æ„å’Œè§’è‰²

```
simple-web-chat/
â”œâ”€â”€ frontend/                    # React å‰ç«¯
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # UIç»„ä»¶
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat.tsx       # èŠå¤©ç•Œé¢
â”‚   â”‚   â”‚   â”œâ”€â”€ ContextSelector.tsx  # URLé€‰æ‹©
â”‚   â”‚   â”‚   â””â”€â”€ IngestionForm.tsx    # URLè¾“å…¥è¡¨å•
â”‚   â”‚   â””â”€â”€ App.tsx            # ä¸»åº”ç”¨ç¨‹åº
â”‚   â”œâ”€â”€ package.json           # ä¾èµ–ç®¡ç†
â”‚   â””â”€â”€ vite.config.ts         # æ„å»ºé…ç½®
â”‚
â”œâ”€â”€ backend/                     # Python åç«¯
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/endpoints/  # APIç«¯ç‚¹
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # èŠå¤©API
â”‚   â”‚   â”‚   â”œâ”€â”€ contexts.py   # ä¸Šä¸‹æ–‡ç®¡ç†
â”‚   â”‚   â”‚   â””â”€â”€ scrape.py     # ç½‘é¡µæŠ“å–
â”‚   â”‚   â”œâ”€â”€ services/          # ä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py      # LLMé›†æˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_db_service.py # å‘é‡æ•°æ®åº“æ“ä½œ
â”‚   â”‚   â”‚   â””â”€â”€ scraping_service.py # æŠ“å–è¿‡ç¨‹
â”‚   â”‚   â””â”€â”€ main.py            # FastAPIåº”ç”¨ç¨‹åº
â”‚   â””â”€â”€ requirements.txt       # Pythonä¾èµ–
â”‚
â”œâ”€â”€ docker-compose.yml           # Dockeré…ç½®
â”œâ”€â”€ .env                        # ç¯å¢ƒå˜é‡
â””â”€â”€ README.md                   # ä¸»è¦æ–‡æ¡£
```

### å…³é”®æ–‡ä»¶è¯´æ˜

#### `frontend/src/App.tsx`
**è§’è‰²**: ä¸»åº”ç”¨ç¨‹åºç•Œé¢
**å­¦ä¹ è¦ç‚¹**: React Hooks, çŠ¶æ€ç®¡ç†, APIè°ƒç”¨

```typescript
// å…³é”®çŠ¶æ€ç®¡ç†
const [contexts, setContexts] = useState<string[]>([])
const [selectedContext, setSelectedContext] = useState<string>('')
const [messages, setMessages] = useState<Message[]>([])

// APIé€šä¿¡æ¨¡å¼
const fetchContexts = async () => {
  const response = await fetch('/api/v1/contexts')
  const data = await response.json()
  setContexts(data.contexts)
}
```

#### `backend/app/services/llm_service.py`
**è§’è‰²**: ä¸Ollama LLMé›†æˆ
**å­¦ä¹ è¦ç‚¹**: ä¸AIæ¨¡å‹é€šä¿¡, æç¤ºå·¥ç¨‹

```python
def generate_chat_response(self, query: str, context: str) -> str:
    # è®¾è®¡ç³»ç»Ÿæç¤ºï¼ˆé‡è¦ï¼ï¼‰
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œåªæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
    
    ä¸Šä¸‹æ–‡:
    {context}
    
    è¯´æ˜:
    - åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯æ¥å›ç­”ã€‚
    - å¦‚æœæ‰¾ä¸åˆ°ä¿¡æ¯ï¼Œè¯·å¦‚å®å›ç­”â€œæˆ‘ä¸çŸ¥é“â€ã€‚
    """
```

#### `backend/app/services/vector_db_service.py`
**è§’è‰²**: å‘é‡æ•°æ®åº“æ“ä½œ
**å­¦ä¹ è¦ç‚¹**: åµŒå…¥ç”Ÿæˆ, ç›¸ä¼¼æ€§æœç´¢

```python
def similarity_search(self, query: str, context_id: str, top_k: int = 5):
    # 1. å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
    query_vector = self.ollama_service.generate_embedding(query)
    
    # 2. æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
    search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
    results = self.collection.search(
        data=[query_vector],
        anns_field="vector",
        param=search_params,
        limit=top_k
    )
```

## æŠ€æœ¯æ ˆè¯¦è§£

### å‰ç«¯: React + TypeScript + Vite

#### ä¸ºä½•é€‰æ‹©
- **React**: åŸºäºç»„ä»¶çš„è®¾è®¡ï¼Œä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ
- **TypeScript**: ç±»å‹å®‰å…¨ï¼Œé€‚åˆå¤§è§„æ¨¡å¼€å‘
- **Vite**: å¿«é€Ÿçš„å¼€å‘æœåŠ¡å™¨ï¼Œç°ä»£åŒ–çš„æ„å»ºå·¥å…·

#### å…³é”®æŠ€æœ¯æ¦‚å¿µ
```typescript
// 1. ä½¿ç”¨React Hooksè¿›è¡ŒçŠ¶æ€ç®¡ç†
const [isLoading, setIsLoading] = useState<boolean>(false)

// 2. ä½¿ç”¨useEffectå¤„ç†å‰¯ä½œç”¨
useEffect(() => {
  fetchContexts()
}, []) // ä»…åœ¨åˆå§‹æ¸²æŸ“æ—¶è¿è¡Œ

// 3. ä½¿ç”¨TypeScriptè¿›è¡Œç±»å‹å®šä¹‰
interface Message {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
}
```

### åç«¯: FastAPI + Python

#### ä¸ºä½•é€‰æ‹©
- **FastAPI**: è‡ªåŠ¨ç”ŸæˆAPIæ–‡æ¡£, ç±»å‹æç¤ºåˆ©ç”¨, é«˜æ€§èƒ½
- **Python**: ä¸°å¯Œçš„AI/MLåº“, å¯è¯»æ€§

#### å…³é”®æ¨¡å¼
```python
# 1. ä¾èµ–æ³¨å…¥æ¨¡å¼
@router.post("/chat")
async def chat(
    request: ChatRequest,
    llm_service: LLMService = Depends(get_llm_service)
):
    return await llm_service.generate_response(request.message)

# 2. ä½¿ç”¨Pydanticè¿›è¡Œæ•°æ®éªŒè¯
class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=1000)
    context_id: str = Field(..., regex="^[a-zA-Z0-9_-]+$")
```

### æ•°æ®åº“: Milvuså‘é‡æ•°æ®åº“

#### ä¸ºä½•é€‰æ‹©
- **é«˜æ€§èƒ½**: æ”¯æŒæ•°åäº¿å‘é‡çš„æœç´¢
- **å¯æ‰©å±•æ€§**: é€šè¿‡é›†ç¾¤é…ç½®è¿›è¡Œæ°´å¹³æ‰©å±•
- **å…¼å®¹æ€§**: æ”¯æŒæ ‡å‡†çš„å‘é‡æ•°æ®åº“æ“ä½œ

#### å…³é”®æ“ä½œ
```python
# 1. åˆ›å»ºä¸€ä¸ªé›†åˆï¼ˆæ¨¡å¼å®šä¹‰ï¼‰
schema = CollectionSchema(
    fields=[
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1024),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=1000)
    ]
)

# 2. åˆ›å»ºç´¢å¼•ï¼ˆç”¨äºæœç´¢åŠ é€Ÿï¼‰
index_params = {
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024},
    "metric_type": "COSINE"
}
collection.create_index(field_name="vector", index_params=index_params)
```

## å‰ç«¯è¯¦è§£

### ç»„ä»¶è®¾è®¡æ¨¡å¼

#### 1. Chat.tsx - å®ç°èŠå¤©åŠŸèƒ½
```typescript
export function Chat({ selectedContext }: ChatProps) {
  const [messages, setMessages] = useState<Message[]>([])
  const [input, setInput] = useState('')
  const [isLoading, setIsLoading] = useState(false)

  // ä½¿ç”¨WebSocketæˆ–æœåŠ¡å™¨å‘é€äº‹ä»¶è¿›è¡Œå®æ—¶é€šä¿¡
  const sendMessage = async (message: string) => {
    setIsLoading(true)
    
    try {
      const response = await fetch('/api/v1/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message,
          context_id: selectedContext,
          conversation_history: messages
        })
      })
      
      const data = await response.json()
      setMessages(prev => [...prev, 
        { role: 'user', content: message, timestamp: new Date() },
        { role: 'assistant', content: data.response, timestamp: new Date() }
      ])
    } catch (error) {
      console.error('Chat error:', error)
    } finally {
      setIsLoading(false)
    }
  }
}
```

#### 2. IngestionForm.tsx - å®ç°URLå¤„ç†
```typescript
const handleSubmit = async (url: string) => {
  // 1. URLéªŒè¯
  if (!isValidUrl(url)) {
    setError('è¯·è¾“å…¥æœ‰æ•ˆçš„URL')
    return
  }

  // 2. é‡å¤æ£€æŸ¥
  if (contexts.includes(url)) {
    setError('æ­¤URLå·²è¢«å¤„ç†')
    return
  }

  // 3. åœ¨åç«¯æ‰§è¡ŒæŠ“å–
  setIsProcessing(true)
  try {
    await fetch('/api/v1/scrape', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ url })
    })
    
    // 4. æ›´æ–°ä¸Šä¸‹æ–‡åˆ—è¡¨
    await refreshContexts()
    setSuccess('ç½‘ç«™å¤„ç†å®Œæˆ')
  } catch (error) {
    setError('å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯')
  } finally {
    setIsProcessing(false)
  }
}
```

### çŠ¶æ€ç®¡ç†è®¾è®¡åŸåˆ™

```typescript
// 1. å•ä¸€èŒè´£åŸåˆ™ - æ¯ä¸ªç»„ä»¶ä¸“æ³¨äºä¸€ä¸ªåŠŸèƒ½
// 2. æ¸…æ™°çš„æ•°æ®æµ - props down, events up
// 3. é€‚å½“çš„å‰¯ä½œç”¨ç®¡ç† - useEffectçš„ç²¾ç¡®ä¾èµ–æ•°ç»„

// å¥½çš„ä¾‹å­: æ¸…æ™°çš„èŒè´£åˆ†ç¦»
function App() {
  const [contexts, setContexts] = useState<string[]>([])
  const [selectedContext, setSelectedContext] = useState<string>('')
  
  return (
    <div>
      <IngestionForm onNewContext={handleNewContext} />
      <ContextSelector 
        contexts={contexts} 
        selected={selectedContext}
        onSelect={setSelectedContext} 
      />
      <Chat selectedContext={selectedContext} />
    </div>
  )
}
```

## åç«¯APIè¯¦è§£

### FastAPIè®¾è®¡æ¨¡å¼

#### 1. è·¯ç”±å™¨é…ç½®
```python
# app/api/v1/endpoints/chat.py
from fastapi import APIRouter, Depends, HTTPException
from app.services.llm_service import LLMService, get_llm_service

router = APIRouter(prefix="/chat", tags=["chat"])

@router.post("/")
async def chat_endpoint(
    request: ChatRequest,
    llm_service: LLMService = Depends(get_llm_service)
) -> ChatResponse:
    """
    èŠå¤©API
    
    - **message**: ç”¨æˆ·çš„é—®é¢˜
    - **context_id**: å¯¹è¯ä¸Šä¸‹æ–‡ID
    - **conversation_history**: è¿‡å»çš„å¯¹è¯å†å²
    """
    try:
        response = await llm_service.generate_chat_response(
            query=request.message,
            context_id=request.context_id,
            history=request.conversation_history
        )
        return ChatResponse(response=response)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

#### 2. æœåŠ¡å±‚å®ç°
```python
# app/services/llm_service.py
class LLMService:
    def __init__(self):
        self.client = ollama.Client(host=OLLAMA_HOST)
        self.vector_service = VectorDBService()
    
    async def generate_chat_response(
        self, 
        query: str, 
        context_id: str,
        history: List[Message] = None
    ) -> str:
        # 1. ä½¿ç”¨å‘é‡æœç´¢æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.vector_service.similarity_search(
            query=query, 
            context_id=context_id,
            top_k=5
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n".join([doc.content for doc in relevant_docs])
        
        # 3. ç»„è£…æç¤º
        messages = self._build_messages(query, context, history)
        
        # 4. æ‰§è¡ŒLLMæ¨ç†
        response = await self._call_ollama(messages)
        
        return response
    
    def _build_messages(self, query: str, context: str, history: List[Message]):
        """è€ƒè™‘å¯¹è¯å†å²æ„å»ºæ¶ˆæ¯"""
        messages = [{
            "role": "system",
            "content": f"""
            ä½ æ˜¯ä¸€ä¸ªæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜çš„AIã€‚
            
            ä¸Šä¸‹æ–‡:
            {context}
            
            è¯´æ˜:
            - åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯
            - è€ƒè™‘å¯¹è¯æµç¨‹ï¼Œè‡ªç„¶åœ°å›ç­”
            - å¦‚æœä¸çŸ¥é“ï¼Œè¯·å¦‚å®å›ç­”
            """
        }]
        
        # æ·»åŠ å¯¹è¯å†å²
        if history:
            for msg in history[-10:]:  # åªå–æœ€è¿‘10æ¡æ¶ˆæ¯
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # æ·»åŠ å½“å‰é—®é¢˜
        messages.append({"role": "user", "content": query})
        
        return messages
```

### é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

```python
# å®šä¹‰è‡ªå®šä¹‰å¼‚å¸¸
class LLMServiceError(Exception):
    """LLMæœåŠ¡ç›¸å…³é”™è¯¯"""
    pass

class VectorDBError(Exception):
    """å‘é‡æ•°æ®åº“ç›¸å…³é”™è¯¯"""
    pass

# å…¨å±€å¼‚å¸¸å¤„ç†ç¨‹åº
@app.exception_handler(LLMServiceError)
async def llm_service_error_handler(request: Request, exc: LLMServiceError):
    return JSONResponse(
        status_code=500,
        content={
            "error": "LLM_SERVICE_ERROR",
            "message": "ä¸è¯­è¨€æ¨¡å‹é€šä¿¡æ—¶å‘ç”Ÿé”™è¯¯",
            "detail": str(exc)
        }
    )
```

## RAGæŠ€æœ¯è¯¦è§£

### RAGç®¡é“çš„è¯¦ç»†æµç¨‹

```python
class RAGPipeline:
    """RAGå¤„ç†ç®¡é“çš„ç¤ºä¾‹å®ç°"""
    
    def __init__(self):
        self.embedding_model = EmbeddingModel("mxbai-embed-large")
        self.vector_db = VectorDatabase()
        self.llm = LanguageModel("gpt-oss:20b")
    
    async def process_document(self, url: str, content: str):
        """æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–"""
        # 1. æ–‡æœ¬é¢„å¤„ç†
        chunks = self._chunk_text(content, chunk_size=1000, overlap=200)
        
        # 2. å‘é‡åŒ–æ¯ä¸ªå—
        vectors = []
        for chunk in chunks:
            vector = await self.embedding_model.encode(chunk)
            vectors.append({
                'id': self._generate_id(),
                'text': chunk,
                'vector': vector,
                'url': url,
                'metadata': self._extract_metadata(chunk)
            })
        
        # 3. ä¿å­˜åˆ°å‘é‡æ•°æ®åº“
        await self.vector_db.insert(vectors)
    
    def _chunk_text(self, text: str, chunk_size: int, overlap: int):
        """å°†æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å—"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk = text[start:end]
            
            # è°ƒæ•´ä»¥é¿å…åœ¨å¥å­ä¸­é—´åˆ‡æ–­
            if end < len(text):
                last_sentence = chunk.rfind('ã€‚')
                if last_sentence > chunk_size * 0.8:
                    end = start + last_sentence + 1
                    chunk = text[start:end]
            
            chunks.append(chunk)
            start = end - overlap
        
        return chunks
    
    async def retrieve_and_generate(self, query: str, context_id: str):
        """é›†æˆçš„æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹"""
        # 1. æ‰©å±•æœç´¢æŸ¥è¯¢
        expanded_query = await self._expand_query(query)
        
        # 2. å‘é‡æœç´¢
        search_results = await self.vector_db.similarity_search(
            query=expanded_query,
            context_id=context_id,
            top_k=10
        )
        
        # 3. å¯¹ç»“æœè¿›è¡Œé‡æ–°æ’åº
        reranked_results = self._rerank_results(query, search_results)
        
        # 4. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(reranked_results[:5])
        
        # 5. ç”Ÿæˆ
        response = await self.llm.generate(
            prompt=self._build_prompt(query, context),
            max_tokens=500,
            temperature=0.7
        )
        
        return response
    
    def _rerank_results(self, query: str, results: List[SearchResult]):
        """è€ƒè™‘å¤šæ ·æ€§å’Œåˆ†æ•°çš„æœç´¢ç»“æœé‡æ–°æ’åº"""
        scored_results = []
        
        for result in results:
            # è¯­ä¹‰ç›¸ä¼¼åº¦
            semantic_score = result.similarity_score
            
            # å…³é”®å­—åŒ¹é…åˆ†æ•°
            keyword_score = self._calculate_keyword_score(query, result.text)
            
            # å¤šæ ·æ€§åˆ†æ•°ï¼ˆä¸å·²é€‰ç»“æœçš„é‡å åº¦ï¼‰
            diversity_score = self._calculate_diversity_score(result, scored_results)
            
            # æ€»åˆ†
            total_score = (
                semantic_score * 0.5 +
                keyword_score * 0.3 +
                diversity_score * 0.2
            )
            
            scored_results.append((result, total_score))
        
        return [result for result, _ in sorted(scored_results, key=lambda x: x[1], reverse=True)]
```

### æç¤ºå·¥ç¨‹æœ€ä½³å®è·µ

```python
class PromptTemplate:
    """æœ‰æ•ˆçš„æç¤ºæ¨¡æ¿"""
    
    SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è¯´æ˜ï¼š

ã€è§’è‰²ã€‘
- åªæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜
- å¼ºè°ƒå‡†ç¡®æ€§å’Œå®ç”¨æ€§
- ç†è§£å¯¹è¯æµç¨‹å¹¶è‡ªç„¶å›åº”

ã€çº¦æŸã€‘
- ä¸ä½¿ç”¨ä¸Šä¸‹æ–‡ä¹‹å¤–çš„çŸ¥è¯†
- é¿å…åŸºäºæ¨æµ‹çš„å›ç­”  
- å¦‚æœä¸çŸ¥é“ï¼Œè¯·å¦‚å®å›ç­”â€œæˆ‘æ‰¾ä¸åˆ°ä¿¡æ¯â€
- æ˜ç¡®è¯´æ˜å›ç­”çš„ä¾æ®

ã€å›åº”æ ¼å¼ã€‘
- ç®€æ´æ˜“æ‡‚çš„ä¸­æ–‡
- æ ¹æ®éœ€è¦ä½¿ç”¨é¡¹ç›®ç¬¦å·æˆ–ç»“æ„åŒ–
- å¼ºè°ƒé‡ç‚¹
"""
    
    USER_PROMPT_TEMPLATE = """
ä¸Šä¸‹æ–‡:
---
{context}
---

å¯¹è¯å†å²:
{conversation_history}

é—®é¢˜: {query}

å›ç­”:"""
    
    @classmethod
    def build_messages(cls, query: str, context: str, history: List[dict]):
        """æ„å»ºå®Œæ•´çš„æ¶ˆæ¯"""
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history_text = ""
        if history:
            for msg in history[-5:]:  # æœ€è¿‘5æ¡æ¶ˆæ¯
                role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
                history_text += f"\n{role}: {msg['content']}"
        
        user_prompt = cls.USER_PROMPT_TEMPLATE.format(
            context=context,
            conversation_history=history_text,
            query=query
        )
        
        return [
            {"role": "system", "content": cls.SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]
```

## å‘é‡æ•°æ®åº“è¯¦è§£

### Milvusçš„å†…éƒ¨å·¥ä½œåŸç†

```python
class MilvusManager:
    """Milvusæ“ä½œçš„è¯¦ç»†å®ç°"""
    
    def __init__(self):
        self.connections.connect(
            alias="default",
            host="localhost",
            port="19530"
        )
    
    def create_optimized_collection(self, collection_name: str, dimension: int):
        """åˆ›å»ºä¸€ä¸ªä¼˜åŒ–çš„é›†åˆ"""
        # æ¨¡å¼è®¾è®¡
        fields = [
            FieldSchema(
                name="id", 
                dtype=DataType.INT64, 
                is_primary=True, 
                auto_id=True
            ),
            FieldSchema(
                name="vector", 
                dtype=DataType.FLOAT_VECTOR, 
                dim=dimension
            ),
            FieldSchema(
                name="text", 
                dtype=DataType.VARCHAR, 
                max_length=65535
            ),
            FieldSchema(
                name="url", 
                dtype=DataType.VARCHAR, 
                max_length=1000
            ),
            FieldSchema(
                name="timestamp", 
                dtype=DataType.INT64
            )
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="å…·æœ‰ä¼˜åŒ–æœç´¢çš„RAGæ–‡æ¡£å­˜å‚¨"
        )
        
        collection = Collection(collection_name, schema)
        
        # ç´¢å¼•é…ç½®ï¼ˆç”¨äºæœç´¢æ€§èƒ½ä¼˜åŒ–ï¼‰
        index_params = {
            "metric_type": "COSINE",  # ä½™å¼¦ç›¸ä¼¼åº¦
            "index_type": "IVF_FLAT", # å€’æ’æ–‡ä»¶ç´¢å¼•
            "params": {
                "nlist": 1024  # ç°‡çš„æ•°é‡
            }
        }
        
        collection.create_index(
            field_name="vector",
            index_params=index_params
        )
        
        return collection
    
    async def advanced_search(
        self, 
        collection: Collection,
        query_vector: List[float],
        filters: dict = None,
        top_k: int = 10
    ):
        """é«˜çº§æœç´¢åŠŸèƒ½"""
        # æœç´¢å‚æ•°
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 16}  # è¦æœç´¢çš„ç°‡çš„æ•°é‡
        }
        
        # æ„å»ºè¿‡æ»¤å™¨è¡¨è¾¾å¼
        expr = ""
        if filters:
            conditions = []
            for key, value in filters.items():
                if isinstance(value, str):
                    conditions.append(f'{key} == "{value}"')
                elif isinstance(value, list):
                    conditions.append(f'{key} in {value}')
            expr = " and ".join(conditions)
        
        # æ‰§è¡Œæœç´¢
        results = collection.search(
            data=[query_vector],
            anns_field="vector",
            param=search_params,
            limit=top_k,
            expr=expr if expr else None,
            output_fields=["text", "url", "timestamp"]
        )
        
        return self._process_search_results(results)
    
    def _process_search_results(self, raw_results):
        """åå¤„ç†æœç´¢ç»“æœ"""
        processed_results = []
        
        for hits in raw_results:
            for hit in hits:
                result = SearchResult(
                    id=hit.id,
                    score=hit.score,
                    text=hit.entity.get("text"),
                    url=hit.entity.get("url"),
                    timestamp=hit.entity.get("timestamp")
                )
                processed_results.append(result)
        
        # åˆ†æ•°å½’ä¸€åŒ–
        if processed_results:
            max_score = max(r.score for r in processed_results)
            min_score = min(r.score for r in processed_results)
            score_range = max_score - min_score
            
            for result in processed_results:
                if score_range > 0:
                    result.normalized_score = (result.score - min_score) / score_range
                else:
                    result.normalized_score = 1.0
        
        return processed_results
```

### åµŒå…¥æ¨¡å‹é€‰æ‹©å’Œä¼˜åŒ–

```python
class EmbeddingOptimizer:
    """åµŒå…¥æ¨¡å‹ä¼˜åŒ–"""
    
    def __init__(self, model_name: str = "mxbai-embed-large"):
        self.model_name = model_name
        self.client = ollama.Client()
        self.cache = {}  # åµŒå…¥ç¼“å­˜
    
    async def generate_embedding(self, text: str) -> List[float]:
        """ä¼˜åŒ–çš„åµŒå…¥ç”Ÿæˆ"""
        # æ£€æŸ¥ç¼“å­˜
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # æ–‡æœ¬é¢„å¤„ç†
        processed_text = self._preprocess_text(text)
        
        # ç”ŸæˆåµŒå…¥
        try:
            response = await self.client.embeddings(
                model=self.model_name,
                prompt=processed_text
            )
            embedding = response["embedding"]
            
            # å½’ä¸€åŒ–
            embedding = self._normalize_vector(embedding)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache[text_hash] = embedding
            
            return embedding
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise EmbeddingError(f"Failed to generate embedding: {e}")
    
    def _preprocess_text(self, text: str) -> str:
        """ç”¨äºåµŒå…¥ç”Ÿæˆçš„æ–‡æœ¬é¢„å¤„ç†"""
        # åˆ é™¤ä¸å¿…è¦çš„å­—ç¬¦å’Œç¬¦å·
        text = re.sub(r'[^\w\s.,!?]', '', text)
        
        # æˆªæ–­é•¿æ–‡æœ¬
        if len(text) > 8000:  # mxbai-embed-largeçš„é™åˆ¶
            text = text[:8000]
        
        # è§„èŒƒåŒ–ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _normalize_vector(self, vector: List[float]) -> List[float]:
        """è§„èŒƒåŒ–å‘é‡ï¼ˆåˆ°å•ä½å‘é‡ï¼‰"""
        import numpy as np
        norm = np.linalg.norm(vector)
        return (np.array(vector) / norm).tolist() if norm > 0 else vector
```

## åˆå­¦è€…å®šåˆ¶

### çº§åˆ«1: UIå®šåˆ¶

```typescript
// frontend/src/App.tsxä¸­ç®€å•æ›´æ”¹çš„ç¤ºä¾‹

// 1. æ›´æ”¹æ ‡é¢˜
const APP_TITLE = "æˆ‘çš„AIèŠå¤©åŠ©æ‰‹" // åŸ: "RAG Chat Application"

// 2. æ›´æ”¹é¢œè‰²ä¸»é¢˜
const theme = {
  primary: "#4f46e5",      // åŸ: "#3b82f6" 
  secondary: "#10b981",    // åŸ: "#6b7280"
  background: "#f8fafc",   // åŸ: "#ffffff"
  text: "#1f2937"         // åŸ: "#374151"
}

// 3. æ›´æ”¹å ä½ç¬¦æ–‡æœ¬
const PLACEHOLDER_MESSAGES = {
  urlInput: "è¾“å…¥æ‚¨æƒ³å­¦ä¹ çš„ç½‘ç«™çš„URL...",
  chatInput: "è¾“å…¥æ‚¨çš„é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šè¿™ç¯‡æ–‡ç« çš„è¦ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿï¼‰...",
  noContext: "è¯·å…ˆæå–ä¸€ä¸ªç½‘ç«™"
}

// 4. è‡ªå®šä¹‰æ¶ˆæ¯æ˜¾ç¤º
function Message({ message, isUser }: MessageProps) {
  return (
    <div className={`message ${isUser ? 'user' : 'assistant'}`}>
      {!isUser && <span className="avatar">ğŸ¤–</span>}
      <div className="content">
        {message.content}
        <span className="timestamp">
          {message.timestamp.toLocaleTimeString('zh-CN')}
        </span>
      </div>
      {isUser && <span className="avatar">ğŸ‘¤</span>}
    </div>
  )
}
```

### çº§åˆ«2: åŠŸèƒ½æ·»åŠ 

```python
# å‘backend/app/api/v1/endpoints/chat.pyæ·»åŠ åŠŸèƒ½çš„ç¤ºä¾‹

@router.post("/summarize")
async def summarize_context(
    request: SummarizeRequest,
    llm_service: LLMService = Depends(get_llm_service)
) -> SummarizeResponse:
    """ä¸Šä¸‹æ–‡æ‘˜è¦åŠŸèƒ½"""
    try:
        # è·å–æŒ‡å®šä¸Šä¸‹æ–‡çš„å…¨éƒ¨å†…å®¹
        all_content = await vector_service.get_all_content(request.context_id)
        
        # æ„å»ºæ‘˜è¦æç¤º
        summary_prompt = f"""
è¯·ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹ç½‘ç«™çš„å†…å®¹ï¼š

å†…å®¹:
{all_content[:4000]}  # è€ƒè™‘ä»¤ç‰Œé™åˆ¶

è¦æ±‚:
- 3-5ä¸ªè¦ç‚¹ï¼Œä»¥é¡¹ç›®ç¬¦å·åˆ—è¡¨å½¢å¼
- æ¯ä¸ªè¦ç‚¹ç®€æ´ï¼Œ1-2å¥è¯
- åŒ…æ‹¬é‡è¦çš„æ•°å­—å’Œä¸“æœ‰åè¯
"""
        
        summary = await llm_service.generate_summary(summary_prompt)
        
        return SummarizeResponse(
            summary=summary,
            context_id=request.context_id,
            generated_at=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)}")

# æ–°æ•°æ®æ¨¡å‹
class SummarizeRequest(BaseModel):
    context_id: str = Field(..., description="è¦æ‘˜è¦çš„ä¸Šä¸‹æ–‡ID")

class SummarizeResponse(BaseModel):
    summary: str = Field(..., description="ç”Ÿæˆçš„æ‘˜è¦")
    context_id: str = Field(..., description="åŸå§‹ä¸Šä¸‹æ–‡ID")
    generated_at: datetime = Field(..., description="ç”Ÿæˆæ—¶é—´æˆ³")
```

### çº§åˆ«3: é«˜çº§å®šåˆ¶

```python
# app/services/advanced_rag_service.py
class AdvancedRAGService:
    """é«˜çº§RAGåŠŸèƒ½çš„å®ç°"""
    
    def __init__(self):
        self.llm_service = LLMService()
        self.vector_service = VectorDBService()
        self.query_analyzer = QueryAnalyzer()
    
    async def multi_step_reasoning(self, query: str, context_id: str):
        """å¤šæ­¥æ¨ç†çš„å®ç°"""
        
        # 1. åˆ†æå’Œåˆ†è§£æŸ¥è¯¢
        sub_queries = await self.query_analyzer.decompose_query(query)
        
        # 2. å¯¹æ¯ä¸ªå­æŸ¥è¯¢è¿›è¡Œæœç´¢å’Œå›ç­”
        sub_answers = []
        for sub_query in sub_queries:
            docs = await self.vector_service.similarity_search(sub_query, context_id)
            answer = await self.llm_service.generate_response(sub_query, docs)
            sub_answers.append({
                'query': sub_query,
                'answer': answer,
                'sources': docs
            })
        
        # 3. ç”Ÿæˆç»¼åˆç­”æ¡ˆ
        final_prompt = self._build_integration_prompt(query, sub_answers)
        final_answer = await self.llm_service.generate_response(final_prompt)
        
        return {
            'final_answer': final_answer,
            'reasoning_steps': sub_answers,
            'confidence_score': self._calculate_confidence(sub_answers)
        }
    
    async def conversational_search(self, query: str, conversation_history: List[dict]):
        """è€ƒè™‘å¯¹è¯å†å²çš„æœç´¢"""
        
        # 1. ä»å¯¹è¯å†å²ä¸­æå–ä¸Šä¸‹æ–‡å…³é”®å­—
        context_keywords = self._extract_context_keywords(conversation_history)
        
        # 2. ä¸Šä¸‹æ–‡æŸ¥è¯¢
        contextualized_query = await self._contextualize_query(query, context_keywords)
        
        # 3. æ‰§è¡Œæ‰©å±•æœç´¢
        search_results = await self.vector_service.contextual_search(
            query=contextualized_query,
            context_keywords=context_keywords,
            top_k=15
        )
        
        # 4. ç”Ÿæˆé€‚åˆå¯¹è¯çš„å›ç­”
        response = await self.llm_service.conversational_response(
            query=query,
            documents=search_results,
            conversation_history=conversation_history
        )
        
        return response
```

## è¯¦ç»†æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 1. Ollamaè¿æ¥é”™è¯¯
```bash
# é”™è¯¯: "Could not connect to Ollama"

# è¯Šæ–­æ­¥éª¤:
# 1. æ£€æŸ¥Ollamaè¿›ç¨‹
ps aux | grep ollama

# 2. æ£€æŸ¥ç«¯å£
lsof -i :11434

# 3. æ£€æŸ¥Ollamaæ—¥å¿—
ollama serve --verbose

# 4. æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
ollama list

# è§£å†³æ–¹æ¡ˆ:
# å¦‚æœOllamaæœªè¿è¡Œ
ollama serve &

# å¦‚æœç«¯å£æ­£åœ¨ä½¿ç”¨
sudo lsof -ti:11434 | xargs sudo kill -9
ollama serve --port 11435  # åœ¨ä¸åŒç«¯å£ä¸Šå¯åŠ¨
```

#### 2. Dockerå†…å­˜ç›¸å…³é”™è¯¯
```bash
# é”™è¯¯: Dockerå®¹å™¨æ— æ³•å¯åŠ¨

# è¯Šæ–­:
docker system info | grep -E "Total Memory|CPUs"
docker system df
docker stats

# è§£å†³æ–¹æ¡ˆ:
# åˆ é™¤æœªä½¿ç”¨çš„å®¹å™¨å’Œé•œåƒ
docker system prune -a

# é‡å¯Dockerå®ˆæŠ¤è¿›ç¨‹
sudo systemctl restart docker  # Linux
# é‡å¯Docker Desktop # macOS/Windows
```

#### 3. å‰ç«¯æ„å»ºé”™è¯¯
```bash
# é”™è¯¯: "Module not found" æˆ– "Build failed"

# æ£€æŸ¥Node.js/npmç‰ˆæœ¬
node --version  # æ¨è: v18+
npm --version

# é‡æ–°å®‰è£…ä¾èµ–é¡¹
cd frontend
rm -rf node_modules package-lock.json
npm install

# æ£€æŸ¥TypeScripté…ç½®
npx tsc --noEmit  # ç±»å‹æ£€æŸ¥

# æ£€æŸ¥Viteé…ç½®
npm run build -- --debug
```

### æ€§èƒ½ä¼˜åŒ–

#### æ¨¡å‹é€‰æ‹©æŒ‡å—

```python
# æ¯ä¸ªç¯å¢ƒçš„æ¨èè®¾ç½®
PERFORMANCE_CONFIGS = {
    "development": {
        "generation_model": "tinyllama",
        "embedding_model": "mxbai-embed-large", 
        "vector_search_top_k": 3,
        "max_context_length": 2000
    },
    "staging": {
        "generation_model": "gpt-oss:20b",
        "embedding_model": "mxbai-embed-large",
        "vector_search_top_k": 5,
        "max_context_length": 4000
    },
    "production": {
        "generation_model": "gpt-oss:20b", 
        "embedding_model": "mxbai-embed-large",
        "vector_search_top_k": 10,
        "max_context_length": 6000,
        "enable_caching": True,
        "enable_async_processing": True
    }
}

# åŠ¨æ€é…ç½®åŠ è½½
def get_config():
    env = os.getenv("ENVIRONMENT", "development")
    return PERFORMANCE_CONFIGS.get(env, PERFORMANCE_CONFIGS["development"])
```

## åº”ç”¨ç³»ç»Ÿè®¾è®¡

### å¯æ‰©å±•çš„æ‰©å±•æ–¹æ³•

#### 1. å¾®æœåŠ¡
```yaml
# ç¤ºä¾‹ docker-compose.production.yml
version: '3.8'

services:
  # APIç½‘å…³
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf

  # è®¤è¯æœåŠ¡  
  auth-service:
    build: ./services/auth
    environment:
      - JWT_SECRET=${JWT_SECRET}
      - DATABASE_URL=${AUTH_DB_URL}

  # RAGå¤„ç†æœåŠ¡
  rag-service:
    build: ./services/rag
    replicas: 3
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - VECTOR_DB_HOST=${VECTOR_DB_HOST}

  # ç½‘é¡µæŠ“å–æœåŠ¡
  scraping-service:
    build: ./services/scraping
    environment:
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}

  # é˜Ÿåˆ—å·¥ä½œè€…
  celery-worker:
    build: ./services/scraping
    command: celery worker -A scraping_service.celery
    replicas: 2
```

#### 2. åˆ†å¸ƒå¼å‘é‡æ•°æ®åº“é…ç½®
```python
class DistributedVectorDB:
    """åˆ†å¸ƒå¼å‘é‡æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, nodes: List[str]):
        self.nodes = nodes
        self.load_balancer = LoadBalancer(nodes)
        self.replication_factor = 2
    
    async def insert_with_replication(self, vectors: List[dict]):
        """å¸¦å¤åˆ¶çš„æ•°æ®æ’å…¥"""
        primary_node = self.load_balancer.get_primary_node()
        replica_nodes = self.load_balancer.get_replica_nodes(self.replication_factor)
        
        # æ’å…¥ä¸»èŠ‚ç‚¹
        await self._insert_to_node(primary_node, vectors)
        
        # å¼‚æ­¥å¤åˆ¶åˆ°å‰¯æœ¬èŠ‚ç‚¹
        replication_tasks = [
            self._insert_to_node(node, vectors) 
            for node in replica_nodes
        ]
        await asyncio.gather(*replication_tasks, return_exceptions=True)
    
    async def distributed_search(self, query_vector: List[float], top_k: int):
        """æ‰§è¡Œåˆ†å¸ƒå¼æœç´¢"""
        search_tasks = [
            self._search_on_node(node, query_vector, top_k) 
            for node in self.nodes
        ]
        
        # ä»æ‰€æœ‰èŠ‚ç‚¹æ”¶é›†ç»“æœ
        all_results = await asyncio.gather(*search_tasks)
        
        # åˆå¹¶å’Œæ’åºç»“æœä»¥è¿”å›å‰kä¸ª
        merged_results = self._merge_and_rank_results(all_results, top_k)
        
        return merged_results
```

#### 3. æ·»åŠ å®æ—¶åŠŸèƒ½
```typescript
// åŸºäºWebSocketçš„å®æ—¶èŠå¤©
class RealtimeChatClient {
  private ws: WebSocket
  private messageQueue: Message[] = []
  
  constructor(wsUrl: string) {
    this.ws = new WebSocket(wsUrl)
    this.setupEventHandlers()
  }
  
  setupEventHandlers() {
    this.ws.onmessage = (event) => {
      const data = JSON.parse(event.data)
      
      switch (data.type) {
        case 'chat_response_chunk':
          this.handleStreamingResponse(data.chunk)
          break
        case 'processing_status':
          this.updateProcessingStatus(data.status)
          break
        case 'error':
          this.handleError(data.error)
          break
      }
    }
  }
  
  async sendMessage(message: string, contextId: string) {
    const payload = {
      type: 'chat_message',
      message,
      context_id: contextId,
      timestamp: Date.now()
    }
    
    this.ws.send(JSON.stringify(payload))
  }
  
  private handleStreamingResponse(chunk: string) {
    // æ›´æ–°æµå¼å“åº”çš„æ˜¾ç¤º
    const currentMessage = this.getCurrentMessage()
    if (currentMessage) {
      currentMessage.content += chunk
      this.updateMessageDisplay(currentMessage)
    }
  }
}
```

## å­¦ä¹ èµ„æºå’Œä¸‹ä¸€æ­¥

### æŒ‰æŠ€æœ¯æ ˆåˆ’åˆ†çš„å­¦ä¹ è·¯å¾„

#### React + TypeScript
- **å®˜æ–¹æ–‡æ¡£**: [React Docs](https://react.dev/)
- **TypeScriptå­¦ä¹ **: [TypeScript Handbook](https://www.typescriptlang.org/docs/)
- **å®è·µé¡¹ç›®**: æ‰©å±•æ­¤ä»£ç åº“çš„å‰ç«¯

#### FastAPI + Python
- **å®˜æ–¹æ–‡æ¡£**: [FastAPI](https://fastapi.tiangolo.com/)
- **å¼‚æ­¥ç¼–ç¨‹**: [Asyncio in Python](https://docs.python.org/3/library/asyncio.html)
- **å®è·µé¡¹ç›®**: æ‰©å±•åç«¯APIåŠŸèƒ½

#### å‘é‡æ•°æ®åº“
- **Milvuså®˜æ–¹**: [Milvus Documentation](https://milvus.io/docs)
- **å‘é‡æœç´¢ç†è®º**: çº¿æ€§ä»£æ•°ã€ä¿¡æ¯æ£€ç´¢åŸºç¡€
- **å®è·µé¡¹ç›®**: è¯•éªŒæé«˜æœç´¢å‡†ç¡®æ€§

#### LLMå’ŒRAG
- **Ollama**: [Ollama Documentation](https://ollama.ai/docs)
- **RAGè®ºæ–‡**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- **æç¤ºå·¥ç¨‹**: è®¾è®¡æœ‰æ•ˆçš„æç¤º

### é«˜çº§é¡¹ç›®æ„æƒ³

#### åˆçº§é¡¹ç›®
1. **UIæ”¹è¿›**: å¼•å…¥è®¾è®¡ç³»ç»Ÿï¼Œå“åº”å¼è®¾è®¡
2. **åŠŸèƒ½æ·»åŠ **: æ”¶è—å¤¹ã€æœç´¢å†å²ã€å¯¼å‡ºåŠŸèƒ½
3. **è¯­è¨€æ”¯æŒ**: å¤šè¯­è¨€UIã€ç¿»è¯‘åŠŸèƒ½
4. **æ€§èƒ½**: åŠ è½½çŠ¶æ€ã€æ”¹è¿›çš„é”™è¯¯å¤„ç†

#### ä¸­çº§é¡¹ç›®  
1. **è®¤è¯ç³»ç»Ÿ**: ç”¨æˆ·ç®¡ç†ã€ä¼šè¯ç®¡ç†
2. **æ–‡ä»¶ä¸Šä¼ **: æå–PDFã€Wordæ–‡æ¡£
3. **é«˜çº§æœç´¢**: è¿‡æ»¤ã€æ’åºã€åˆ†é¢æœç´¢
4. **åˆ†æåŠŸèƒ½**: ä½¿ç”¨ç»Ÿè®¡ã€çƒ­é—¨å†…å®¹åˆ†æ

#### é«˜çº§é¡¹ç›®
1. **å¤šç§Ÿæˆ·**: å°†å…¶è½¬å˜ä¸ºé¢å‘ä¼ä¸šçš„SaaS
2. **å…¬å…±API**: ç¬¬ä¸‰æ–¹é›†æˆã€Webhook
3. **ML Ops**: æ¨¡å‹æ€§èƒ½ç›‘æ§ã€A/Bæµ‹è¯•
4. **åˆ†å¸ƒå¼ç³»ç»Ÿ**: å¾®æœåŠ¡ã€Kubernetesæ”¯æŒ

### æ•…éšœæ’é™¤ç¤¾åŒº

- **GitHub Issues**: åœ¨æ­¤ä»£ç åº“çš„Issuesé€‰é¡¹å¡ä¸­æé—®
- **Discord/Slack**: åœ¨AIå¼€å‘ç¤¾åŒºä¸­äº¤æµä¿¡æ¯
- **Stack Overflow**: è§£å†³æŠ€æœ¯é—®é¢˜
- **Reddit**: åœ¨r/MachineLearningã€r/webdevä¸Šè®¨è®º

---

## æ€»ç»“

é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å¯ä»¥äº†è§£ç°ä»£AI Webåº”ç”¨ç¨‹åºçš„æ•´ä½“æƒ…å†µï¼Œå¹¶ä»ä¸€ä¸ªå·¥ä½œçš„ç³»ç»Ÿä¸­å­¦ä¹ ï¼Œä»ç†è®ºå’Œå®è·µä¸¤æ–¹é¢è·å¾—æŠ€èƒ½ã€‚

**æœ€é‡è¦çš„æ˜¯**:
1. **åŠ¨æ‰‹å®è·µ**: é¦–å…ˆè®©å®ƒå·¥ä½œï¼Œç„¶åè¿›è¡Œå®šåˆ¶
2. **æ¸è¿›å¼å­¦ä¹ **: æ ¹æ®æ‚¨çš„æ°´å¹³é€æ­¥æé«˜æŠ€èƒ½
3. **ç¤¾åŒºå‚ä¸**: ä¸å…¶ä»–å­¦ä¹ è€…äº¤æµä¿¡æ¯

æŠ€æœ¯åœ¨ä¸æ–­å‘å±•ï¼Œå› æ­¤åŸºäºæ­¤ä»£ç åº“å¼€å‘è‡ªå·±çš„é¡¹ç›®æ˜¯å­¦ä¹ çš„æœ€æœ‰æ•ˆæ–¹æ³•ã€‚

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
- [ ] ç¡®è®¤åŸºæœ¬æ“ä½œ
- [ ] æ ¹æ®æ‚¨çš„æ°´å¹³å®æ–½å®šåˆ¶  
- [ ] è§„åˆ’å’Œå®æ–½ä¸€ä¸ªé«˜çº§é¡¹ç›®
- [ ] ä¸ç¤¾åŒºåˆ†äº«æ‚¨å­¦åˆ°çš„ä¸œè¥¿

ç¥ä½ å¥½è¿ï¼ğŸš€