# RAG-based Web Content Chat Application

This project is a full-stack web application that allows you to scrape the content of multiple websites, store them in a partitioned vector database, and then chat with the content of a specific site using a Retrieval-Augmented Generation (RAG) model.

The application is built with a modern stack, containerized for easy setup, and provides a simple, clean user interface.

---

# æ—¥æœ¬èªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€è¤‡æ•°ã®Webã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–ã‚Šè¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã€ç‰¹å®šã®ã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ãƒãƒ£ãƒƒãƒˆã§ãã‚‹RAGï¼ˆRetrieval-Augmented Generationï¼‰ãƒ™ãƒ¼ã‚¹ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚

## ğŸš€ ã‹ã‚“ãŸã‚“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### å‰ææ¡ä»¶

- **Docker Desktop** ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã“ã¨
- **ãƒ¡ãƒ¢ãƒªè¦ä»¶**ï¼ˆMacãƒ¦ãƒ¼ã‚¶ãƒ¼é‡è¦ï¼‰ï¼š
  - `gpt-oss:20b`ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚: æœ€ä½16GB RAMæ¨å¥¨
  - `tinyllama`ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨æ™‚: 4GB RAMï¼ˆè»½é‡ç‰ˆï¼‰

### 1. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
git clone <repository-url>
cd <repository-directory>
```

### 2. Docker Desktopã®ãƒ¡ãƒ¢ãƒªè¨­å®šç¢ºèªï¼ˆMacãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰

ç¾åœ¨ã®Dockerå‰²ã‚Šå½“ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¢ºèªï¼š
```bash
docker system info | grep "Total Memory"
```

16GBæœªæº€ã®å ´åˆã€Docker Desktopã®è¨­å®šã§ãƒ¡ãƒ¢ãƒªã‚’å¢—ã‚„ã™ã‹ã€è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

### 3. ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼š

```bash
# é«˜å“è³ªãƒ¢ãƒ‡ãƒ«ï¼ˆ16GB+ Docker ãƒ¡ãƒ¢ãƒªå¿…è¦ï¼‰
GENERATION_MODEL=gpt-oss:20b

# è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆ4GB Docker ãƒ¡ãƒ¢ãƒªã§å‹•ä½œã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
GENERATION_MODEL=tinyllama
```

### 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š

```bash
docker compose up --build
```

### 5. åˆå›èµ·å‹•æ™‚ã®æ³¨æ„äº‹é …

åˆå›èµ·å‹•æ™‚ã«ã¯è‡ªå‹•çš„ã«LLMãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼š
- `gpt-oss:20b`: ç´„13GBã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- `tinyllama`: ç´„638MBã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰  
- `mxbai-embed-large`: ç´„669MBã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

é€²è¡ŒçŠ¶æ³ã¯ä»¥ä¸‹ã§ç¢ºèªã§ãã¾ã™ï¼š
```bash
docker compose logs -f backend
```

### 6. ã‚¢ã‚¯ã‚»ã‚¹

å…¨ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ãŸã‚‰ã€Webãƒ–ãƒ©ã‚¦ã‚¶ã§ä»¥ä¸‹ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼š
**http://localhost:5173**

## ğŸ¯ ä½¿ã„æ–¹

### 1. Webã‚µã‚¤ãƒˆã®å–ã‚Šè¾¼ã¿

- ã€Œ1. Ingest a Websiteã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€å‡¦ç†ã—ãŸã„Webã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›
- ä¾‹: `https://ja.wikipedia.org/wiki/äººå·¥çŸ¥èƒ½`
- ã€ŒProcess URLã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
- è¤‡æ•°ã®URLã‚’å‡¦ç†å¯èƒ½

### 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é¸æŠ

- Webã‚µã‚¤ãƒˆã‚’å–ã‚Šè¾¼ã‚€ã¨ã€ŒSelect a context to chat withã€ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ãŒè¡¨ç¤º
- ãƒãƒ£ãƒƒãƒˆã—ãŸã„Webã‚µã‚¤ãƒˆã‚’é¸æŠ

### 3. ãƒãƒ£ãƒƒãƒˆ

- ã€Œ2. Chatã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§é¸æŠã—ãŸWebã‚µã‚¤ãƒˆã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã¤ã„ã¦è³ªå•
- è³ªå•ã‚’å…¥åŠ›ã—ã¦Enterã‚­ãƒ¼ã¾ãŸã¯é€ä¿¡ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
- é¸æŠã•ã‚ŒãŸWebã‚µã‚¤ãƒˆã®æƒ…å ±ã®ã¿ã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆ

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

1. **Docker ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã®ç¢ºèª:**
   ```bash
   docker system info | grep "Total Memory"
   ```

2. **ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆ:**
   - **æ–¹æ³•1**: Docker Desktopã®ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã‚’å¢—åŠ 
   - **æ–¹æ³•2**: è»½é‡ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆ`.env`ã§`GENERATION_MODEL=tinyllama`ã«å¤‰æ›´ï¼‰

3. **è¨­å®šå¤‰æ›´å¾Œã®å†èµ·å‹•:**
   ```bash
   docker compose down
   docker compose up -d
   ```

### ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ

- **gpt-oss:20b**: é«˜å“è³ªãªå›ç­”ã€å¤§å®¹é‡ãƒ¡ãƒ¢ãƒªå¿…è¦ï¼ˆ13.4GB+ï¼‰
- **tinyllama**: è»½é‡ç‰ˆï¼ˆ638MBï¼‰ã€ãƒ†ã‚¹ãƒˆã‚„åˆ¶é™ç’°å¢ƒå‘ã‘

---

## Features

- **Web Scraping**: Ingest content from any number of URLs.
- **Multi-Context Chat**: Each ingested URL creates a separate context. You can switch between contexts to chat with a specific website's content.
- **Vector Embeddings**: Uses Ollama with the `mxbai-embed-large` model to generate embeddings for the web content.
- **Partitioned Vector Storage**: Stores text and embeddings in a local Milvus vector database, with each URL's content stored in a separate partition for data isolation.
- **RAG Chat**: Uses Ollama with the `gpt-oss:20b` model to answer questions based on the stored content.
- **Web UI**: A modern React frontend for ingesting websites and interacting with the chat.

## Tech Stack

- **Backend**: FastAPI (Python)
- **Frontend**: React (TypeScript) with Vite and `shadcn/ui`
- **Vector Database**: Milvus (v2.3.10)
- **LLM Serving**: Ollama
  - **Embedding Model**: `mxbai-embed-large`
  - **Generation Model**: Configurable
    - `gpt-oss:20b` - Full model (requires 16GB+ Docker memory)
    - `tinyllama` - Lightweight model (works with 4GB Docker memory)
- **Containerization**: Docker and Docker Compose

## Prerequisites

- [Docker](https://www.docker.com/get-started/) installed and running on your machine.
- **Important for Mac users**: Docker Desktop must have sufficient memory allocated:
  - **For gpt-oss:20b model**: Minimum 16GB RAM (recommended)
  - **For tinyllama model**: 4GB RAM (lightweight alternative)
  - See [Docker Memory Setup Guide](./DOCKER_MEMORY_SETUP.md) for detailed instructions

## Getting Started

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Configure Docker Desktop Memory (Mac users):**
    - Check current Docker memory: `docker system info | grep "Total Memory"`
    - If less than 16GB, see [Docker Memory Setup Guide](./DOCKER_MEMORY_SETUP.md)
    - For limited memory environments, use the lightweight model (see Model Configuration below)

3.  **Model Configuration (Optional):**
    The application uses environment variables to select the LLM model:
    - Edit `.env` file to choose your model:
      ```bash
      # For systems with 16GB+ Docker memory:
      GENERATION_MODEL=gpt-oss:20b
      
      # For systems with limited memory (default):
      GENERATION_MODEL=tinyllama
      ```

4.  **Start the application:**
    Run the following command in the root of the project directory:
    ```bash
    docker-compose up --build
    ```
    This command will build the Docker images for the frontend and backend services and start all the required containers (Ollama, Milvus, Backend, Frontend).

5.  **Initial Model Download (First-time setup):**
    The first time you start the application, the backend service will automatically pull the required LLM models. The download size depends on your chosen model:
    - `gpt-oss:20b`: ~13GB download
    - `tinyllama`: ~638MB download
    - `mxbai-embed-large`: ~669MB download
    
    You can monitor the progress by checking the logs:
    ```bash
    docker-compose logs -f backend
    ```

6.  **Access the application:**
    Once all services are running and the models are downloaded, you can access the web interface by navigating to:
    [http://localhost:5173](http://localhost:5173)

## How to Use

1.  **Ingest a Website**:
    - In the "1. Ingest a Website" section, enter the full URL of a website you want to process (e.g., `https://en.wikipedia.org/wiki/Artificial_intelligence`).
    - Click the "Process URL" button. The application will scrape the content and store it in the database.
    - You can repeat this process for multiple URLs.

2.  **Select a Context**:
    - After ingesting one or more websites, a dropdown menu labeled "Select a context to chat with" will appear.
    - This dropdown will contain all the URLs you have ingested.
    - Select a URL from the list to set it as the context for your conversation.

3.  **Chat with the Content**:
    - Once a context is selected, you can ask questions about its content in the "2. Chat" section.
    - Type your question in the input box and press Enter or click the send button.
    - The application will retrieve the most relevant information *from the selected website only* and use it to generate an answer.
    - The answer and the sources used to generate it will appear in the chat window.

## Services

The `docker-compose.yml` file defines the following services:

- `ollama`: The Ollama server for running the language models.
- `etcd`, `minio`, `milvus`: The Milvus vector database and its dependencies.
- `backend`: The FastAPI application that serves the main API.
- `frontend`: The Vite development server for the React frontend.

## Troubleshooting

### Memory Issues

If you encounter memory errors like "out of memory" when using the gpt-oss model:

1. **Check Docker Desktop memory allocation:**
   ```bash
   docker system info | grep "Total Memory"
   ```

2. **If memory is insufficient:**
   - Option 1: Increase Docker Desktop memory allocation (see [Docker Memory Setup Guide](./DOCKER_MEMORY_SETUP.md))
   - Option 2: Switch to the lightweight `tinyllama` model by editing `.env`:
     ```bash
     GENERATION_MODEL=tinyllama
     ```

3. **After changing memory settings or model:**
   ```bash
   docker-compose down
   docker-compose up -d
   ```

### Model Performance

- **gpt-oss:20b**: Provides the best quality responses but requires significant memory (13.4GB+)
- **tinyllama**: Lightweight alternative (638MB) suitable for testing and memory-constrained environments

## Environment Variables

The application supports the following environment variables (configured in `.env`):

- `GENERATION_MODEL`: LLM model for text generation (default: `tinyllama`)
- `EMBEDDING_MODEL`: Model for generating embeddings (default: `mxbai-embed-large`)
