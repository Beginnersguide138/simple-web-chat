# åˆå¿ƒè€…å‘ã‘è©³ç´°ã‚¬ã‚¤ãƒ‰ - RAGãƒ™ãƒ¼ã‚¹Webãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

> ğŸ‘‹ **ã“ã®ã‚¬ã‚¤ãƒ‰ã«ã¤ã„ã¦**: åˆå¿ƒè€…ã®AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã«ã€ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’é€šã˜ã¦ç¾ä»£çš„ãªAIæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã‚’å­¦ç¿’ã§ãã‚‹ã‚ˆã†è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ¯ ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å­¦ã¹ã‚‹ã“ã¨

### æŠ€è¡“é ˜åŸŸ
- **RAG (Retrieval-Augmented Generation)**: AIãƒãƒ£ãƒƒãƒˆã®æœ€æ–°æ‰‹æ³•
- **ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«æ™‚ä»£ã®é‡è¦æŠ€è¡“
- **ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯é–‹ç™º**: React + FastAPI ã®ãƒ¢ãƒ€ãƒ³ãªçµ„ã¿åˆã‚ã›
- **ã‚³ãƒ³ãƒ†ãƒŠæŠ€è¡“**: Docker ã‚’ä½¿ã£ãŸå®Ÿç”¨çš„ãªé–‹ç™ºç’°å¢ƒæ§‹ç¯‰
- **LLMé‹ç”¨**: Ollama ã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ç®¡ç†

### ã‚¹ã‚­ãƒ«ç¿’å¾—ç›®æ¨™
1. **AIæŠ€è¡“ã®å®Ÿè·µçš„ç†è§£**: ç†è«–ã§ã¯ãªãã€å‹•ä½œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å­¦ç¿’
2. **ãƒ¢ãƒ€ãƒ³Webé–‹ç™º**: TypeScript + React ã®æœ€æ–°ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é–‹ç™º
3. **APIè¨­è¨ˆ**: FastAPI ã‚’ä½¿ã£ãŸREST APIã®è¨­è¨ˆã¨å®Ÿè£…
4. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ**: ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¦‚å¿µã¨æ´»ç”¨
5. **DevOpsåŸºç¤**: Docker ã«ã‚ˆã‚‹é–‹ç™ºç’°å¢ƒã®æ¨™æº–åŒ–

## ğŸ“š å‰æçŸ¥è­˜ãƒ¬ãƒ™ãƒ«åˆ¥å­¦ç¿’ãƒ‘ã‚¹

### ğŸŸ¢ ãƒ¬ãƒ™ãƒ«1: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°åˆå¿ƒè€…
**å¿…è¦ãªåŸºç¤çŸ¥è­˜**: åŸºæœ¬çš„ãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°æ¦‚å¿µï¼ˆå¤‰æ•°ã€é–¢æ•°ã€æ¡ä»¶åˆ†å²ï¼‰

**å­¦ç¿’é †åº**:
1. [ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å‹•ä½œç¢ºèª](#step1-å‹•ä½œç¢ºèª)
2. [åŸºæœ¬æ¦‚å¿µã®ç†è§£](#åŸºæœ¬æ¦‚å¿µè§£èª¬)
3. [ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®æŠŠæ¡](#ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ è©³è§£)
4. [ç°¡å˜ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º](#åˆå¿ƒè€…å‘ã‘ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º)

### ğŸŸ¡ ãƒ¬ãƒ™ãƒ«2: Webé–‹ç™ºçµŒé¨“è€…
**å‰æçŸ¥è­˜**: HTML/CSS/JavaScript ã®åŸºæœ¬ã€RESTAPIã®æ¦‚å¿µ

**å­¦ç¿’é †åº**:
1. [æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯æ¦‚è¦](#æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è©³è§£)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç†è§£](#ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
3. [ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è§£æ](#ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è©³è§£)
4. [ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIè§£æ](#ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰apiè©³è§£)

### ğŸŸ  ãƒ¬ãƒ™ãƒ«3: AI/MLåˆå­¦è€…
**å‰æçŸ¥è­˜**: Pythonã€åŸºæœ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’æ¦‚å¿µ

**å­¦ç¿’é †åº**:
1. [RAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç†è§£](#ragæŠ€è¡“è©³è§£)
2. [ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ´»ç”¨](#ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³è§£)
3. [LLMçµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³](#llmçµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³)
4. [å¿œç”¨ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ](#å¿œç”¨ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ)

## Step1: å‹•ä½œç¢ºèª

### 1.1 ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆè©³ç´°ç‰ˆï¼‰

#### macOSç’°å¢ƒã§ã®è©³ç´°æ‰‹é †

```bash
# 1. Homebrewã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆï¼‰
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# 2. å¿…è¦ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
brew install git docker ollama

# 3. Docker Desktop ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆGUIãŒå¿…è¦ï¼‰
open https://www.docker.com/products/docker-desktop/

# 4. ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone <ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®URL>
cd simple-web-chat
```

#### Windowsç’°å¢ƒã§ã®è©³ç´°æ‰‹é †

```powershell
# 1. ChocolateyçµŒç”±ã§ãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆç®¡ç†è€…æ¨©é™ï¼‰
Set-ExecutionPolicy Bypass -Scope Process -Force
iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))

choco install git docker-desktop

# 2. Ollamaã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://ollama.ai/ ã‹ã‚‰Windowsç‰ˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

# 3. ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone <ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®URL>
cd simple-web-chat
```

### 1.2 åˆå›èµ·å‹•ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```bash
# âœ… Docker Desktop ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
docker --version
docker compose version

# âœ… Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
ollama serve &
ollama list

# âœ… å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰
ollama pull mxbai-embed-large    # ç´„669MB
ollama pull tinyllama            # ç´„638MBï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰

# âœ… ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
docker compose up --build

# âœ… ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹
open http://localhost:5173
```

### 1.3 å‹•ä½œç¢ºèªæ‰‹é †

1. **Webã‚µã‚¤ãƒˆå–ã‚Šè¾¼ã¿ãƒ†ã‚¹ãƒˆ**
   ```
   URLä¾‹: https://ja.wikipedia.org/wiki/äººå·¥çŸ¥èƒ½
   â†’ ã€ŒProcess URLã€ã‚¯ãƒªãƒƒã‚¯
   â†’ æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèª
   ```

2. **ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ**
   ```
   ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¸æŠ: å…ˆç¨‹å–ã‚Šè¾¼ã‚“ã URL
   è³ªå•ä¾‹: "äººå·¥çŸ¥èƒ½ã®å®šç¾©ã¯ä½•ã§ã™ã‹ï¼Ÿ"
   â†’ AIãŒå›ç­”ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
   ```

## åŸºæœ¬æ¦‚å¿µè§£èª¬

### RAG (Retrieval-Augmented Generation) ã¨ã¯ï¼Ÿ

**å¾“æ¥ã®ãƒãƒ£ãƒƒãƒˆbot**:
```
ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå• â†’ LLM â†’ å›ç­”
```
- å•é¡Œ: LLMã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ç„¡ã„æƒ…å ±ã¯ç­”ãˆã‚‰ã‚Œãªã„
- å•é¡Œ: å¤ã„æƒ…å ±ã‚„ä¸æ­£ç¢ºãªæƒ…å ±ã‚’å›ç­”ã™ã‚‹å¯èƒ½æ€§

**RAGã‚·ã‚¹ãƒ†ãƒ **:
```
ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå• â†’ é–¢é€£æ–‡æ›¸æ¤œç´¢ â†’ LLMï¼ˆè³ªå•ï¼‹æ–‡æ›¸ï¼‰ â†’ å›ç­”
```
- è§£æ±º: æœ€æ–°ã®æ­£ç¢ºãªæ–‡æ›¸ã«åŸºã¥ã„ã¦å›ç­”
- è§£æ±º: ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®å°‚é–€çŸ¥è­˜ã‚’æ´»ç”¨å¯èƒ½

### ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å½¹å‰²

**å¾“æ¥ã®æ¤œç´¢**:
```sql
SELECT * FROM documents WHERE content LIKE '%äººå·¥çŸ¥èƒ½%'
```
- å•é¡Œ: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå®Œå…¨ä¸€è‡´ã—ãªã„ã¨æ¤œç´¢ã§ããªã„
- å•é¡Œ: æ„å‘³çš„ã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œãªã„

**ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢**:
```python
# ã€ŒAIã€ã€Œæ©Ÿæ¢°å­¦ç¿’ã€ã€Œãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãŒæ„å‘³çš„ã«è¿‘ã„ã¨åˆ¤æ–­
query_vector = embedding_model("äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦")
similar_docs = vector_db.similarity_search(query_vector, top_k=5)
```

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³è§£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰   â”‚â”€â”€â”€â–¶â”‚   ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰API   â”‚â”€â”€â”€â–¶â”‚  Ollama (LLM)   â”‚
â”‚   (React/TS)    â”‚    â”‚   (FastAPI)      â”‚    â”‚  (ãƒ›ã‚¹ãƒˆãƒã‚·ãƒ³)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Milvus Vector DB â”‚
                    â”‚   (Docker)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ è©³è§£

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã¨å½¹å‰²

```
simple-web-chat/
â”œâ”€â”€ frontend/                    # React ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat.tsx       # ãƒãƒ£ãƒƒãƒˆç”»é¢
â”‚   â”‚   â”‚   â”œâ”€â”€ ContextSelector.tsx  # URLé¸æŠ
â”‚   â”‚   â”‚   â””â”€â”€ IngestionForm.tsx    # URLå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
â”‚   â”‚   â””â”€â”€ App.tsx            # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
â”‚   â”œâ”€â”€ package.json           # ä¾å­˜é–¢ä¿‚ç®¡ç†
â”‚   â””â”€â”€ vite.config.ts         # ãƒ“ãƒ«ãƒ‰è¨­å®š
â”‚
â”œâ”€â”€ backend/                    # Python ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/endpoints/  # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # ãƒãƒ£ãƒƒãƒˆAPI
â”‚   â”‚   â”‚   â”œâ”€â”€ contexts.py   # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
â”‚   â”‚   â”‚   â””â”€â”€ scrape.py     # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
â”‚   â”‚   â”œâ”€â”€ services/          # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py      # LLMé€£æº
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_db_service.py # ãƒ™ã‚¯ãƒˆãƒ«DBæ“ä½œ
â”‚   â”‚   â”‚   â””â”€â”€ scraping_service.py # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â””â”€â”€ requirements.txt       # Pythonä¾å­˜é–¢ä¿‚
â”‚
â”œâ”€â”€ docker-compose.yml          # Dockeræ§‹æˆ
â”œâ”€â”€ .env                       # ç’°å¢ƒå¤‰æ•°
â””â”€â”€ README.md                  # ãƒ¡ã‚¤ãƒ³èª¬æ˜æ›¸
```

### é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«è§£èª¬

#### `frontend/src/App.tsx`
**å½¹å‰²**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³ç”»é¢
**å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**: React Hooksã€çŠ¶æ…‹ç®¡ç†ã€APIå‘¼ã³å‡ºã—

```typescript
// é‡è¦ãªçŠ¶æ…‹ç®¡ç†
const [contexts, setContexts] = useState<string[]>([])
const [selectedContext, setSelectedContext] = useState<string>('')
const [messages, setMessages] = useState<Message[]>([])

// APIã¨ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³
const fetchContexts = async () => {
  const response = await fetch('/api/v1/contexts')
  const data = await response.json()
  setContexts(data.contexts)
}
```

#### `backend/app/services/llm_service.py`
**å½¹å‰²**: Ollama LLMã¨ã®é€£æº
**å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**: AI ãƒ¢ãƒ‡ãƒ«ã¨ã®é€šä¿¡ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
def generate_chat_response(self, query: str, context: str) -> str:
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­è¨ˆï¼ˆé‡è¦ï¼ï¼‰
    system_prompt = f"""
    ã‚ãªãŸã¯è³ªå•ã«å¯¾ã—ã¦ã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
    {context}
    
    æŒ‡ç¤º:
    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”
    - æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯æ­£ç›´ã«ã€Œã‚ã‹ã‚‰ãªã„ã€ã¨ç­”ãˆã‚‹
    """
```

#### `backend/app/services/vector_db_service.py`
**å½¹å‰²**: ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
**å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ**: åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã€é¡ä¼¼æ€§æ¤œç´¢

```python
def similarity_search(self, query: str, context_id: str, top_k: int = 5):
    # 1. ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
    query_vector = self.ollama_service.generate_embedding(query)
    
    # 2. é¡ä¼¼æ€§æ¤œç´¢å®Ÿè¡Œ
    search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
    results = self.collection.search(
        data=[query_vector],
        anns_field="vector",
        param=search_params,
        limit=top_k
    )
```

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è©³è§£

### ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: React + TypeScript + Vite

#### é¸æŠç†ç”±
- **React**: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ™ãƒ¼ã‚¹è¨­è¨ˆã€è±Šå¯Œãªã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 
- **TypeScript**: å‹å®‰å…¨æ€§ã€å¤§è¦æ¨¡é–‹ç™ºå‘ã‘
- **Vite**: é«˜é€Ÿãªé–‹ç™ºã‚µãƒ¼ãƒãƒ¼ã€ãƒ¢ãƒ€ãƒ³ãªãƒ“ãƒ«ãƒ‰ãƒ„ãƒ¼ãƒ«

#### é‡è¦ãªæŠ€è¡“æ¦‚å¿µ
```typescript
// 1. React Hooks ã«ã‚ˆã‚‹çŠ¶æ…‹ç®¡ç†
const [isLoading, setIsLoading] = useState<boolean>(false)

// 2. useEffect ã«ã‚ˆã‚‹å‰¯ä½œç”¨å‡¦ç†
useEffect(() => {
  fetchContexts()
}, []) // åˆå›ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚ã®ã¿å®Ÿè¡Œ

// 3. TypeScript ã®å‹å®šç¾©
interface Message {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
}
```

### ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: FastAPI + Python

#### é¸æŠç†ç”±
- **FastAPI**: è‡ªå‹•APIæ–‡æ›¸ç”Ÿæˆã€å‹ãƒ’ãƒ³ãƒˆæ´»ç”¨ã€é«˜æ€§èƒ½
- **Python**: AI/MLãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è±Šå¯Œã•ã€å¯èª­æ€§

#### é‡è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# 1. ä¾å­˜æ€§æ³¨å…¥ãƒ‘ã‚¿ãƒ¼ãƒ³
@router.post("/chat")
async def chat(
    request: ChatRequest,
    llm_service: LLMService = Depends(get_llm_service)
):
    return await llm_service.generate_response(request.message)

# 2. Pydantic ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=1000)
    context_id: str = Field(..., regex="^[a-zA-Z0-9_-]+$")
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: Milvus Vector Database

#### é¸æŠç†ç”±
- **é«˜æ€§èƒ½**: æ•°å„„è¦æ¨¡ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«å¯¾å¿œ
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹æˆã§æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- **äº’æ›æ€§**: æ¨™æº–çš„ãªãƒ™ã‚¯ãƒˆãƒ«DBæ“ä½œã‚’ã‚µãƒãƒ¼ãƒˆ

#### é‡è¦ãªæ“ä½œ
```python
# 1. ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚¹ã‚­ãƒ¼ãƒå®šç¾©ï¼‰
schema = CollectionSchema(
    fields=[
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1024),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=1000)
    ]
)

# 2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆæ¤œç´¢é«˜é€ŸåŒ–ï¼‰
index_params = {
    "index_type": "IVF_FLAT",
    "params": {"nlist": 1024},
    "metric_type": "COSINE"
}
collection.create_index(field_name="vector", index_params=index_params)
```

## ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰è©³è§£

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

#### 1. Chat.tsx - ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®å®Ÿè£…
```typescript
export function Chat({ selectedContext }: ChatProps) {
  const [messages, setMessages] = useState<Message[]>([])
  const [input, setInput] = useState('')
  const [isLoading, setIsLoading] = useState(false)

  // WebSocket ã¾ãŸã¯ Server-Sent Events ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡
  const sendMessage = async (message: string) => {
    setIsLoading(true)
    
    try {
      const response = await fetch('/api/v1/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message,
          context_id: selectedContext,
          conversation_history: messages
        })
      })
      
      const data = await response.json()
      setMessages(prev => [...prev, 
        { role: 'user', content: message, timestamp: new Date() },
        { role: 'assistant', content: data.response, timestamp: new Date() }
      ])
    } catch (error) {
      console.error('Chat error:', error)
    } finally {
      setIsLoading(false)
    }
  }
}
```

#### 2. IngestionForm.tsx - URLå‡¦ç†ã®å®Ÿè£…
```typescript
const handleSubmit = async (url: string) => {
  // 1. URLå¦¥å½“æ€§æ¤œè¨¼
  if (!isValidUrl(url)) {
    setError('æœ‰åŠ¹ãªURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„')
    return
  }

  // 2. é‡è¤‡ãƒã‚§ãƒƒã‚¯
  if (contexts.includes(url)) {
    setError('ã“ã®URLã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™')
    return
  }

  // 3. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
  setIsProcessing(true)
  try {
    await fetch('/api/v1/scrape', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ url })
    })
    
    // 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸€è¦§ã‚’æ›´æ–°
    await refreshContexts()
    setSuccess('Webã‚µã‚¤ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ')
  } catch (error) {
    setError('å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ')
  } finally {
    setIsProcessing(false)
  }
}
```

### çŠ¶æ…‹ç®¡ç†ã®è¨­è¨ˆåŸå‰‡

```typescript
// 1. å˜ä¸€è²¬ä»»ã®åŸå‰‡ - å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ä¸€ã¤ã®æ©Ÿèƒ½ã«é›†ä¸­
// 2. ãƒ‡ãƒ¼ã‚¿ã®æµã‚Œã‚’æ˜ç¢ºã«ã™ã‚‹ - props down, events up
// 3. å‰¯ä½œç”¨ã‚’é©åˆ‡ã«ç®¡ç† - useEffect ã®ä¾å­˜é…åˆ—ã‚’æ­£ç¢ºã«

// è‰¯ã„ä¾‹: æ˜ç¢ºãªè²¬ä»»åˆ†é›¢
function App() {
  const [contexts, setContexts] = useState<string[]>([])
  const [selectedContext, setSelectedContext] = useState<string>('')
  
  return (
    <div>
      <IngestionForm onNewContext={handleNewContext} />
      <ContextSelector 
        contexts={contexts} 
        selected={selectedContext}
        onSelect={setSelectedContext} 
      />
      <Chat selectedContext={selectedContext} />
    </div>
  )
}
```

## ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIè©³è§£

### FastAPI ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

#### 1. ãƒ«ãƒ¼ã‚¿ãƒ¼æ§‹æˆ
```python
# app/api/v1/endpoints/chat.py
from fastapi import APIRouter, Depends, HTTPException
from app.services.llm_service import LLMService, get_llm_service

router = APIRouter(prefix="/chat", tags=["chat"])

@router.post("/")
async def chat_endpoint(
    request: ChatRequest,
    llm_service: LLMService = Depends(get_llm_service)
) -> ChatResponse:
    """
    ãƒãƒ£ãƒƒãƒˆAPI
    
    - **message**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
    - **context_id**: å¯¾è©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆID
    - **conversation_history**: éå»ã®ä¼šè©±å±¥æ­´
    """
    try:
        response = await llm_service.generate_chat_response(
            query=request.message,
            context_id=request.context_id,
            history=request.conversation_history
        )
        return ChatResponse(response=response)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

#### 2. ã‚µãƒ¼ãƒ“ã‚¹å±¤ã®å®Ÿè£…
```python
# app/services/llm_service.py
class LLMService:
    def __init__(self):
        self.client = ollama.Client(host=OLLAMA_HOST)
        self.vector_service = VectorDBService()
    
    async def generate_chat_response(
        self, 
        query: str, 
        context_id: str,
        history: List[Message] = None
    ) -> str:
        # 1. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§é–¢é€£æ–‡æ›¸ã‚’å–å¾—
        relevant_docs = self.vector_service.similarity_search(
            query=query, 
            context_id=context_id,
            top_k=5
        )
        
        # 2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        context = "\n".join([doc.content for doc in relevant_docs])
        
        # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
        messages = self._build_messages(query, context, history)
        
        # 4. LLMæ¨è«–å®Ÿè¡Œ
        response = await self._call_ollama(messages)
        
        return response
    
    def _build_messages(self, query: str, context: str, history: List[Message]):
        """ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰"""
        messages = [{
            "role": "system",
            "content": f"""
            ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹AIã§ã™ã€‚
            
            ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
            {context}
            
            æŒ‡ç¤º:
            - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®æƒ…å ±ã®ã¿ä½¿ç”¨
            - ä¼šè©±ã®æµã‚Œã‚’è€ƒæ…®ã—ã¦è‡ªç„¶ã«å›ç­”
            - ä¸æ˜ãªå ´åˆã¯æ­£ç›´ã«ç­”ãˆã‚‹
            """
        }]
        
        # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
        if history:
            for msg in history[-10:]:  # ç›´è¿‘10ä»¶ã®ã¿
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # ç¾åœ¨ã®è³ªå•ã‚’è¿½åŠ 
        messages.append({"role": "user", "content": query})
        
        return messages
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

```python
# ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã®å®šç¾©
class LLMServiceError(Exception):
    """LLMã‚µãƒ¼ãƒ“ã‚¹é–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

class VectorDBError(Exception):
    """ãƒ™ã‚¯ãƒˆãƒ«DBé–¢é€£ã®ã‚¨ãƒ©ãƒ¼"""
    pass

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
@app.exception_handler(LLMServiceError)
async def llm_service_error_handler(request: Request, exc: LLMServiceError):
    return JSONResponse(
        status_code=500,
        content={
            "error": "LLM_SERVICE_ERROR",
            "message": "è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã®é€šä¿¡ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
            "detail": str(exc)
        }
    )
```

## RAGæŠ€è¡“è©³è§£

### RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è©³ç´°ãƒ•ãƒ­ãƒ¼

```python
class RAGPipeline:
    """RAGå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…ä¾‹"""
    
    def __init__(self):
        self.embedding_model = EmbeddingModel("mxbai-embed-large")
        self.vector_db = VectorDatabase()
        self.llm = LanguageModel("gpt-oss:20b")
    
    async def process_document(self, url: str, content: str):
        """æ–‡æ›¸ã®å‡¦ç†ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        # 1. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        chunks = self._chunk_text(content, chunk_size=1000, overlap=200)
        
        # 2. å„ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        vectors = []
        for chunk in chunks:
            vector = await self.embedding_model.encode(chunk)
            vectors.append({
                'id': self._generate_id(),
                'text': chunk,
                'vector': vector,
                'url': url,
                'metadata': self._extract_metadata(chunk)
            })
        
        # 3. ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜
        await self.vector_db.insert(vectors)
    
    def _chunk_text(self, text: str, chunk_size: int, overlap: int):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’é‡è¤‡ã‚ã‚Šã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk = text[start:end]
            
            # æ–‡ç« ã®é€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†èª¿æ•´
            if end < len(text):
                last_sentence = chunk.rfind('ã€‚')
                if last_sentence > chunk_size * 0.8:
                    end = start + last_sentence + 1
                    chunk = text[start:end]
            
            chunks.append(chunk)
            start = end - overlap
        
        return chunks
    
    async def retrieve_and_generate(self, query: str, context_id: str):
        """æ¤œç´¢ã¨ç”Ÿæˆã®çµ±åˆå‡¦ç†"""
        # 1. æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ‹¡å¼µ
        expanded_query = await self._expand_query(query)
        
        # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
        search_results = await self.vector_db.similarity_search(
            query=expanded_query,
            context_id=context_id,
            top_k=10
        )
        
        # 3. çµæœã®å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        reranked_results = self._rerank_results(query, search_results)
        
        # 4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = self._build_context(reranked_results[:5])
        
        # 5. ç”Ÿæˆ
        response = await self.llm.generate(
            prompt=self._build_prompt(query, context),
            max_tokens=500,
            temperature=0.7
        )
        
        return response
    
    def _rerank_results(self, query: str, results: List[SearchResult]):
        """æ¤œç´¢çµæœã®å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå¤šæ§˜æ€§ã¨ã‚¹ã‚³ã‚¢ã‚’è€ƒæ…®ï¼‰"""
        scored_results = []
        
        for result in results:
            # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦
            semantic_score = result.similarity_score
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
            keyword_score = self._calculate_keyword_score(query, result.text)
            
            # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ—¢é¸æŠçµæœã¨ã®é‡è¤‡åº¦ï¼‰
            diversity_score = self._calculate_diversity_score(result, scored_results)
            
            # ç·åˆã‚¹ã‚³ã‚¢
            total_score = (
                semantic_score * 0.5 +
                keyword_score * 0.3 +
                diversity_score * 0.2
            )
            
            scored_results.append((result, total_score))
        
        return [result for result, _ in sorted(scored_results, key=lambda x: x[1], reverse=True)]
```

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

```python
class PromptTemplate:
    """åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
    
    SYSTEM_PROMPT = """
ã‚ãªãŸã¯å°‚é–€çš„ãªæƒ…å ±æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æŒ‡ç¤ºã«å³å¯†ã«å¾“ã£ã¦ãã ã•ã„ï¼š

ã€å½¹å‰²ã€‘
- æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹
- æ­£ç¢ºæ€§ã¨æœ‰ç”¨æ€§ã‚’é‡è¦–ã™ã‚‹
- ä¼šè©±ã®æµã‚Œã‚’ç†è§£ã—è‡ªç„¶ã«å¿œç­”ã™ã‚‹

ã€åˆ¶ç´„ã€‘
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¤–ã®çŸ¥è­˜ã¯ä½¿ç”¨ã—ãªã„
- æ¨æ¸¬ã‚„æ†¶æ¸¬ã§ã®å›ç­”ã¯é¿ã‘ã‚‹  
- ä¸æ˜ãªå ´åˆã¯æ­£ç›´ã«ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹
- å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹éƒ¨åˆ†ã‚’æ˜ç¤ºã™ã‚‹

ã€å¿œç­”å½¢å¼ã€‘
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èª
- å¿…è¦ã«å¿œã˜ã¦ç®‡æ¡æ›¸ãã‚„æ§‹é€ åŒ–
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯å¼·èª¿
"""
    
    USER_PROMPT_TEMPLATE = """
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
---
{context}
---

ä¼šè©±å±¥æ­´:
{conversation_history}

è³ªå•: {query}

å›ç­”:"""
    
    @classmethod
    def build_messages(cls, query: str, context: str, history: List[dict]):
        """å®Œå…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰"""
        # ä¼šè©±å±¥æ­´ã®æ•´å½¢
        history_text = ""
        if history:
            for msg in history[-5:]:  # ç›´è¿‘5ä»¶
                role = "ãƒ¦ãƒ¼ã‚¶ãƒ¼" if msg["role"] == "user" else "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"
                history_text += f"\n{role}: {msg['content']}"
        
        user_prompt = cls.USER_PROMPT_TEMPLATE.format(
            context=context,
            conversation_history=history_text,
            query=query
        )
        
        return [
            {"role": "system", "content": cls.SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]
```

## ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³è§£

### Milvus ã®å†…éƒ¨å‹•ä½œåŸç†

```python
class MilvusManager:
    """Milvusæ“ä½œã®è©³ç´°å®Ÿè£…"""
    
    def __init__(self):
        self.connections.connect(
            alias="default",
            host="localhost",
            port="19530"
        )
    
    def create_optimized_collection(self, collection_name: str, dimension: int):
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ"""
        # ã‚¹ã‚­ãƒ¼ãƒè¨­è¨ˆ
        fields = [
            FieldSchema(
                name="id", 
                dtype=DataType.INT64, 
                is_primary=True, 
                auto_id=True
            ),
            FieldSchema(
                name="vector", 
                dtype=DataType.FLOAT_VECTOR, 
                dim=dimension
            ),
            FieldSchema(
                name="text", 
                dtype=DataType.VARCHAR, 
                max_length=65535
            ),
            FieldSchema(
                name="url", 
                dtype=DataType.VARCHAR, 
                max_length=1000
            ),
            FieldSchema(
                name="timestamp", 
                dtype=DataType.INT64
            )
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="RAG document storage with optimized search"
        )
        
        collection = Collection(collection_name, schema)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šï¼ˆæ¤œç´¢æ€§èƒ½ã®æœ€é©åŒ–ï¼‰
        index_params = {
            "metric_type": "COSINE",  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            "index_type": "IVF_FLAT", # é€†ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            "params": {
                "nlist": 1024  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
            }
        }
        
        collection.create_index(
            field_name="vector",
            index_params=index_params
        )
        
        return collection
    
    async def advanced_search(
        self, 
        collection: Collection,
        query_vector: List[float],
        filters: dict = None,
        top_k: int = 10
    ):
        """é«˜åº¦ãªæ¤œç´¢æ©Ÿèƒ½"""
        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 16}  # æ¤œç´¢ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
        }
        
        # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã®æ§‹ç¯‰
        expr = ""
        if filters:
            conditions = []
            for key, value in filters.items():
                if isinstance(value, str):
                    conditions.append(f'{key} == "{value}"')
                elif isinstance(value, list):
                    conditions.append(f'{key} in {value}')
            expr = " and ".join(conditions)
        
        # æ¤œç´¢å®Ÿè¡Œ
        results = collection.search(
            data=[query_vector],
            anns_field="vector",
            param=search_params,
            limit=top_k,
            expr=expr if expr else None,
            output_fields=["text", "url", "timestamp"]
        )
        
        return self._process_search_results(results)
    
    def _process_search_results(self, raw_results):
        """æ¤œç´¢çµæœã®å¾Œå‡¦ç†"""
        processed_results = []
        
        for hits in raw_results:
            for hit in hits:
                result = SearchResult(
                    id=hit.id,
                    score=hit.score,
                    text=hit.entity.get("text"),
                    url=hit.entity.get("url"),
                    timestamp=hit.entity.get("timestamp")
                )
                processed_results.append(result)
        
        # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
        if processed_results:
            max_score = max(r.score for r in processed_results)
            min_score = min(r.score for r in processed_results)
            score_range = max_score - min_score
            
            for result in processed_results:
                if score_range > 0:
                    result.normalized_score = (result.score - min_score) / score_range
                else:
                    result.normalized_score = 1.0
        
        return processed_results
```

### åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨æœ€é©åŒ–

```python
class EmbeddingOptimizer:
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–"""
    
    def __init__(self, model_name: str = "mxbai-embed-large"):
        self.model_name = model_name
        self.client = ollama.Client()
        self.cache = {}  # åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
    async def generate_embedding(self, text: str) -> List[float]:
        """æœ€é©åŒ–ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        processed_text = self._preprocess_text(text)
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        try:
            response = await self.client.embeddings(
                model=self.model_name,
                prompt=processed_text
            )
            embedding = response["embedding"]
            
            # æ­£è¦åŒ–
            embedding = self._normalize_vector(embedding)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self.cache[text_hash] = embedding
            
            return embedding
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise EmbeddingError(f"Failed to generate embedding: {e}")
    
    def _preprocess_text(self, text: str) -> str:
        """åŸ‹ã‚è¾¼ã¿ç”Ÿæˆç”¨ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        # ä¸è¦ãªæ–‡å­—ã‚„è¨˜å·ã‚’é™¤å»
        text = re.sub(r'[^\w\sã€‚ã€]', '', text)
        
        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šè©°ã‚
        if len(text) > 8000:  # mxbai-embed-large ã®åˆ¶é™
            text = text[:8000]
        
        # ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _normalize_vector(self, vector: List[float]) -> List[float]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã®æ­£è¦åŒ–ï¼ˆå˜ä½ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰"""
        import numpy as np
        norm = np.linalg.norm(vector)
        return (np.array(vector) / norm).tolist() if norm > 0 else vector
```

## åˆå¿ƒè€…å‘ã‘ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ¬ãƒ™ãƒ«1: UI ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```typescript
// frontend/src/App.tsx ã§ã®ç°¡å˜ãªå¤‰æ›´ä¾‹

// 1. ã‚¿ã‚¤ãƒˆãƒ«ã®å¤‰æ›´
const APP_TITLE = "ç§ã®AIãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ" // å…ƒ: "RAG Chat Application"

// 2. è‰²ãƒ†ãƒ¼ãƒã®å¤‰æ›´
const theme = {
  primary: "#4f46e5",      // å…ƒ: "#3b82f6" 
  secondary: "#10b981",    // å…ƒ: "#6b7280"
  background: "#f8fafc",   // å…ƒ: "#ffffff"
  text: "#1f2937"         // å…ƒ: "#374151"
}

// 3. ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®å¤‰æ›´
const PLACEHOLDER_MESSAGES = {
  urlInput: "å­¦ç¿’ã—ãŸã„Webã‚µã‚¤ãƒˆã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
  chatInput: "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã“ã®è¨˜äº‹ã®è¦ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿï¼‰",
  noContext: "ã¾ãšWebã‚µã‚¤ãƒˆã‚’å–ã‚Šè¾¼ã‚“ã§ãã ã•ã„"
}

// 4. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤ºã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
function Message({ message, isUser }: MessageProps) {
  return (
    <div className={`message ${isUser ? 'user' : 'assistant'}`}>
      {!isUser && <span className="avatar">ğŸ¤–</span>}
      <div className="content">
        {message.content}
        <span className="timestamp">
          {message.timestamp.toLocaleTimeString('ja-JP')}
        </span>
      </div>
      {isUser && <span className="avatar">ğŸ‘¤</span>}
    </div>
  )
}
```

### ãƒ¬ãƒ™ãƒ«2: æ©Ÿèƒ½è¿½åŠ 

```python
# backend/app/api/v1/endpoints/chat.py ã¸ã®æ©Ÿèƒ½è¿½åŠ ä¾‹

@router.post("/summarize")
async def summarize_context(
    request: SummarizeRequest,
    llm_service: LLMService = Depends(get_llm_service)
) -> SummarizeResponse:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„æ©Ÿèƒ½"""
    try:
        # æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å…¨æ–‡ã‚’å–å¾—
        all_content = await vector_service.get_all_content(request.context_id)
        
        # è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        summary_prompt = f"""
ä»¥ä¸‹ã®Webã‚µã‚¤ãƒˆã®å†…å®¹ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š

å†…å®¹:
{all_content[:4000]}  # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’è€ƒæ…®

è¦æ±‚:
- ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’3-5å€‹ã®ç®‡æ¡æ›¸ãã§
- å„ãƒã‚¤ãƒ³ãƒˆã¯1-2æ–‡ã§ç°¡æ½”ã«
- é‡è¦ãªæ•°å€¤ã‚„å›ºæœ‰åè©ã¯å«ã‚ã‚‹
"""
        
        summary = await llm_service.generate_summary(summary_prompt)
        
        return SummarizeResponse(
            summary=summary,
            context_id=request.context_id,
            generated_at=datetime.now()
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
class SummarizeRequest(BaseModel):
    context_id: str = Field(..., description="è¦ç´„ã—ãŸã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆID")

class SummarizeResponse(BaseModel):
    summary: str = Field(..., description="ç”Ÿæˆã•ã‚ŒãŸè¦ç´„")
    context_id: str = Field(..., description="å…ƒã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆID")
    generated_at: datetime = Field(..., description="ç”Ÿæˆæ—¥æ™‚")
```

### ãƒ¬ãƒ™ãƒ«3: é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# app/services/advanced_rag_service.py
class AdvancedRAGService:
    """é«˜åº¦ãªRAGæ©Ÿèƒ½ã®å®Ÿè£…"""
    
    def __init__(self):
        self.llm_service = LLMService()
        self.vector_service = VectorDBService()
        self.query_analyzer = QueryAnalyzer()
    
    async def multi_step_reasoning(self, query: str, context_id: str):
        """è¤‡æ•°æ®µéšæ¨è«–ã®å®Ÿè£…"""
        
        # 1. ã‚¯ã‚¨ãƒªåˆ†æã¨åˆ†è§£
        sub_queries = await self.query_analyzer.decompose_query(query)
        
        # 2. å„ã‚µãƒ–ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹æ¤œç´¢ã¨å›ç­”
        sub_answers = []
        for sub_query in sub_queries:
            docs = await self.vector_service.similarity_search(sub_query, context_id)
            answer = await self.llm_service.generate_response(sub_query, docs)
            sub_answers.append({
                'query': sub_query,
                'answer': answer,
                'sources': docs
            })
        
        # 3. çµ±åˆå›ç­”ã®ç”Ÿæˆ
        final_prompt = self._build_integration_prompt(query, sub_answers)
        final_answer = await self.llm_service.generate_response(final_prompt)
        
        return {
            'final_answer': final_answer,
            'reasoning_steps': sub_answers,
            'confidence_score': self._calculate_confidence(sub_answers)
        }
    
    async def conversational_search(self, query: str, conversation_history: List[dict]):
        """ä¼šè©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸæ¤œç´¢"""
        
        # 1. ä¼šè©±å±¥æ­´ã‹ã‚‰æ–‡è„ˆã‚’æŠ½å‡º
        context_keywords = self._extract_context_keywords(conversation_history)
        
        # 2. ã‚¯ã‚¨ãƒªã®æ–‡è„ˆåŒ–
        contextualized_query = await self._contextualize_query(query, context_keywords)
        
        # 3. æ‹¡å¼µæ¤œç´¢ã®å®Ÿè¡Œ
        search_results = await self.vector_service.contextual_search(
            query=contextualized_query,
            context_keywords=context_keywords,
            top_k=15
        )
        
        # 4. ä¼šè©±ã«é©ã—ãŸå›ç­”ç”Ÿæˆ
        response = await self.llm_service.conversational_response(
            query=query,
            documents=search_results,
            conversation_history=conversation_history
        )
        
        return response
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è©³ç´°ã‚¬ã‚¤ãƒ‰

### ä¸€èˆ¬çš„ãªå•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼
```bash
# ã‚¨ãƒ©ãƒ¼: "Could not connect to Ollama"

# è¨ºæ–­æ‰‹é †:
# 1. Ollamaãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª
ps aux | grep ollama

# 2. ãƒãƒ¼ãƒˆã®ç¢ºèª
lsof -i :11434

# 3. Ollamaãƒ­ã‚°ã®ç¢ºèª
ollama serve --verbose

# 4. ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®ç¢ºèª
ollama list

# è§£æ±ºç­–:
# OllamaãŒèµ·å‹•ã—ã¦ã„ãªã„å ´åˆ
ollama serve &

# ãƒãƒ¼ãƒˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆ
sudo lsof -ti:11434 | xargs sudo kill -9
ollama serve --port 11435  # åˆ¥ãƒãƒ¼ãƒˆã§èµ·å‹•
```

#### 2. Docker ãƒ¡ãƒ¢ãƒªé–¢é€£ã‚¨ãƒ©ãƒ¼
```bash
# ã‚¨ãƒ©ãƒ¼: Docker containers not starting

# è¨ºæ–­:
docker system info | grep -E "Total Memory|CPUs"
docker system df
docker stats

# è§£æ±º:
# ä¸è¦ãªã‚³ãƒ³ãƒ†ãƒŠãƒ»ã‚¤ãƒ¡ãƒ¼ã‚¸ã®å‰Šé™¤
docker system prune -a

# Dockerãƒ‡ãƒ¼ãƒ¢ãƒ³ã®å†èµ·å‹•
sudo systemctl restart docker  # Linux
# Docker Desktop ã®å†èµ·å‹• # macOS/Windows
```

#### 3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼
```bash
# ã‚¨ãƒ©ãƒ¼: "Module not found" or "Build failed"

# Node.js/npm ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
node --version  # æ¨å¥¨: v18ä»¥ä¸Š
npm --version

# ä¾å­˜é–¢ä¿‚ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cd frontend
rm -rf node_modules package-lock.json
npm install

# TypeScriptè¨­å®šç¢ºèª
npx tsc --noEmit  # å‹ãƒã‚§ãƒƒã‚¯

# Viteè¨­å®šç¢ºèª
npm run build -- --debug
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

```python
# ç’°å¢ƒåˆ¥æ¨å¥¨è¨­å®š
PERFORMANCE_CONFIGS = {
    "development": {
        "generation_model": "tinyllama",
        "embedding_model": "mxbai-embed-large", 
        "vector_search_top_k": 3,
        "max_context_length": 2000
    },
    "staging": {
        "generation_model": "gpt-oss:20b",
        "embedding_model": "mxbai-embed-large",
        "vector_search_top_k": 5,
        "max_context_length": 4000
    },
    "production": {
        "generation_model": "gpt-oss:20b", 
        "embedding_model": "mxbai-embed-large",
        "vector_search_top_k": 10,
        "max_context_length": 6000,
        "enable_caching": True,
        "enable_async_processing": True
    }
}

# å‹•çš„è¨­å®šèª­ã¿è¾¼ã¿
def get_config():
    env = os.getenv("ENVIRONMENT", "development")
    return PERFORMANCE_CONFIGS.get(env, PERFORMANCE_CONFIGS["development"])
```

## å¿œç”¨ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªæ‹¡å¼µæ–¹æ³•

#### 1. ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹åŒ–
```yaml
# docker-compose.production.yml ã®ä¾‹
version: '3.8'

services:
  # API Gateway
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf

  # èªè¨¼ã‚µãƒ¼ãƒ“ã‚¹  
  auth-service:
    build: ./services/auth
    environment:
      - JWT_SECRET=${JWT_SECRET}
      - DATABASE_URL=${AUTH_DB_URL}

  # RAGå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹
  rag-service:
    build: ./services/rag
    replicas: 3
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - VECTOR_DB_HOST=${VECTOR_DB_HOST}

  # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
  scraping-service:
    build: ./services/scraping
    environment:
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL}

  # ã‚­ãƒ¥ãƒ¼ãƒ¯ãƒ¼ã‚«ãƒ¼
  celery-worker:
    build: ./services/scraping
    command: celery worker -A scraping_service.celery
    replicas: 2
```

#### 2. åˆ†æ•£ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹æˆ
```python
class DistributedVectorDB:
    """åˆ†æ•£ãƒ™ã‚¯ãƒˆãƒ«DBç®¡ç†"""
    
    def __init__(self, nodes: List[str]):
        self.nodes = nodes
        self.load_balancer = LoadBalancer(nodes)
        self.replication_factor = 2
    
    async def insert_with_replication(self, vectors: List[dict]):
        """ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ããƒ‡ãƒ¼ã‚¿æŒ¿å…¥"""
        primary_node = self.load_balancer.get_primary_node()
        replica_nodes = self.load_balancer.get_replica_nodes(self.replication_factor)
        
        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒãƒ¼ãƒ‰ã«æŒ¿å…¥
        await self._insert_to_node(primary_node, vectors)
        
        # ãƒ¬ãƒ—ãƒªã‚«ãƒãƒ¼ãƒ‰ã«éåŒæœŸã§è¤‡è£½
        replication_tasks = [
            self._insert_to_node(node, vectors) 
            for node in replica_nodes
        ]
        await asyncio.gather(*replication_tasks, return_exceptions=True)
    
    async def distributed_search(self, query_vector: List[float], top_k: int):
        """åˆ†æ•£æ¤œç´¢ã®å®Ÿè¡Œ"""
        search_tasks = [
            self._search_on_node(node, query_vector, top_k) 
            for node in self.nodes
        ]
        
        # å…¨ãƒãƒ¼ãƒ‰ã‹ã‚‰çµæœã‚’åé›†
        all_results = await asyncio.gather(*search_tasks)
        
        # çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦ä¸Šä½kä»¶ã‚’è¿”å´
        merged_results = self._merge_and_rank_results(all_results, top_k)
        
        return merged_results
```

#### 3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿèƒ½ã®è¿½åŠ 
```typescript
// WebSocketãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒƒãƒˆ
class RealtimeChatClient {
  private ws: WebSocket
  private messageQueue: Message[] = []
  
  constructor(wsUrl: string) {
    this.ws = new WebSocket(wsUrl)
    this.setupEventHandlers()
  }
  
  setupEventHandlers() {
    this.ws.onmessage = (event) => {
      const data = JSON.parse(event.data)
      
      switch (data.type) {
        case 'chat_response_chunk':
          this.handleStreamingResponse(data.chunk)
          break
        case 'processing_status':
          this.updateProcessingStatus(data.status)
          break
        case 'error':
          this.handleError(data.error)
          break
      }
    }
  }
  
  async sendMessage(message: string, contextId: string) {
    const payload = {
      type: 'chat_message',
      message,
      context_id: contextId,
      timestamp: Date.now()
    }
    
    this.ws.send(JSON.stringify(payload))
  }
  
  private handleStreamingResponse(chunk: string) {
    // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¡¨ç¤ºæ›´æ–°
    const currentMessage = this.getCurrentMessage()
    if (currentMessage) {
      currentMessage.content += chunk
      this.updateMessageDisplay(currentMessage)
    }
  }
}
```

## å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯åˆ¥å­¦ç¿’ãƒ‘ã‚¹

#### React + TypeScript
- **å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [React Docs](https://react.dev/)
- **TypeScriptå­¦ç¿’**: [TypeScript Handbook](https://www.typescriptlang.org/docs/)
- **å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚’æ‹¡å¼µ

#### FastAPI + Python
- **å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [FastAPI](https://fastapi.tiangolo.com/)
- **éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°**: [Asyncio in Python](https://docs.python.org/3/library/asyncio.html)
- **å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã®æ©Ÿèƒ½æ‹¡å¼µ

#### ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **Milvuså…¬å¼**: [Milvus Documentation](https://milvus.io/docs)
- **ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç†è«–**: ç·šå½¢ä»£æ•°ã€æƒ…å ±æ¤œç´¢ã®åŸºç¤
- **å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: æ¤œç´¢ç²¾åº¦ã®æ”¹å–„å®Ÿé¨“

#### LLMã¨RAG
- **Ollama**: [Ollama Documentation](https://ollama.ai/docs)
- **RAGè«–æ–‡**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: åŠ¹æœçš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ

### ç™ºå±•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¢ã‚¤ãƒ‡ã‚¢

#### åˆç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
1. **UIæ”¹å–„**: ãƒ‡ã‚¶ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®å°å…¥ã€ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ
2. **æ©Ÿèƒ½è¿½åŠ **: ãŠæ°—ã«å…¥ã‚Šæ©Ÿèƒ½ã€æ¤œç´¢å±¥æ­´ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
3. **è¨€èªå¯¾å¿œ**: å¤šè¨€èªUIã€ç¿»è¨³æ©Ÿèƒ½
4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„

#### ä¸­ç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ  
1. **èªè¨¼ã‚·ã‚¹ãƒ†ãƒ **: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
2. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: PDFã€Wordæ–‡æ›¸ã®å–ã‚Šè¾¼ã¿
3. **é«˜åº¦ãªæ¤œç´¢**: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ã‚½ãƒ¼ãƒˆã€ãƒ•ã‚¡ã‚»ãƒƒãƒˆæ¤œç´¢
4. **åˆ†ææ©Ÿèƒ½**: ä½¿ç”¨çµ±è¨ˆã€äººæ°—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ

#### ä¸Šç´šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
1. **ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆ**: ä¼æ¥­å‘ã‘SaaSåŒ–
2. **APIå…¬é–‹**: ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£é€£æºã€Webhook
3. **ML Ops**: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç›£è¦–ã€A/Bãƒ†ã‚¹ãƒˆ
4. **åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ **: ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã€Kuberneteså¯¾å¿œ

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

- **GitHub Issues**: ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã®Issueã‚¿ãƒ–ã§è³ªå•
- **Discord/Slack**: AIé–‹ç™ºã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§ã®æƒ…å ±äº¤æ›
- **Stack Overflow**: æŠ€è¡“çš„ãªå•é¡Œã®è§£æ±º
- **Reddit**: r/MachineLearning, r/webdev ã§ã®è­°è«–

---

## ã¾ã¨ã‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã‚’é€šã˜ã¦ã€ãƒ¢ãƒ€ãƒ³ãªAI Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å…¨ä½“åƒã‚’ç†è§£ã—ã€å®Ÿéš›ã«å‹•ä½œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ç†è«–ã¨å®Ÿè·µã®ä¸¡é¢ã‹ã‚‰ã‚¹ã‚­ãƒ«ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

**é‡è¦ãªã®ã¯**:
1. **æ‰‹ã‚’å‹•ã‹ã™ã“ã¨**: ã¾ãšå‹•ä½œã•ã›ã¦ã€ãã®å¾Œã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
2. **æ®µéšçš„ãªå­¦ç¿’**: ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦å¾ã€…ã«ã‚¹ã‚­ãƒ«ã‚’å‘ä¸Š
3. **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å‚åŠ **: ä»–ã®å­¦ç¿’è€…ã¨ã®æƒ…å ±äº¤æ›

æŠ€è¡“ã¯å¸¸ã«é€²åŒ–ã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ™ãƒ¼ã‚¹ã«è‡ªåˆ†ãªã‚Šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç™ºå±•ã•ã›ã¦ã„ãã“ã¨ãŒæœ€ã‚‚åŠ¹æœçš„ãªå­¦ç¿’æ–¹æ³•ã§ã™ã€‚

**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
- [ ] åŸºæœ¬å‹•ä½œã®ç¢ºèª
- [ ] è‡ªåˆ†ã®ãƒ¬ãƒ™ãƒ«ã«é©ã—ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã®å®Ÿè¡Œ  
- [ ] ç™ºå±•ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¼ç”»ã¨å®Ÿè£…
- [ ] å­¦ç¿’ã—ãŸå†…å®¹ã‚’ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§ã‚·ã‚§ã‚¢

é ‘å¼µã£ã¦ãã ã•ã„ï¼ğŸš€